{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b583e2ae",
   "metadata": {},
   "source": [
    "# LangChain — Messages, ChatModels, Chains & LCEL\n",
    "# -------------------------------------------------------------\n",
    "# Make sure you have installed all the requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6165a90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# MESSAGES\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# PROMPT TEMPLATES\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "# CHAINING LLMS (OLD VERSION WITHOUT LCEL)\n",
    "from langchain_classic.chains import LLMChain\n",
    "\n",
    "# OUTPUT PARSER\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "\n",
    "# CHAT MODELS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "# EMBEDDINGS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Many chat and embedding models are there in langchain.\n",
    "some of them are opensource and use it for free.\n",
    "And some of them are paid version, make sure to check the models availability and price before using it.\n",
    "\n",
    "Don't forget to create API keys and store it in .env file before using it.\n",
    "\"\"\"\n",
    "\n",
    "os.environ[\"HF_TOKEN\"]=os.getenv(\"HF_TOKEN\")\n",
    "api_key=os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23df494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8e37eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat_ollama=OllamaLLM(model='llama3')\n",
    "\n",
    "\"\"\"\n",
    "Before using the ollama, it needs to be installed in the local system.\n",
    "check it in the documentation for better understanding.\n",
    "\"\"\"\n",
    "chat_groq=ChatGroq(model=\"openai/gpt-oss-20b\",api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084f515b",
   "metadata": {},
   "source": [
    "### Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ddb6830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm doing great, thanks for asking. How can I help you today?\", additional_kwargs={'reasoning_content': 'We need to respond to a friendly greeting. Just say something polite.'}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 86, 'total_tokens': 127, 'completion_time': 0.041136095, 'prompt_time': 0.004291421, 'queue_time': 0.036683126, 'total_time': 0.045427516, 'completion_tokens_details': {'reasoning_tokens': 15}}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e189667b30', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--fd575249-6c6e-41a3-a5e1-1ebaa249ff6b-0', usage_metadata={'input_tokens': 86, 'output_tokens': 41, 'total_tokens': 127})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Common usage of messages with chat models\n",
    "system_msg = SystemMessage(\"You are a helpful assistant.\")\n",
    "human_msg = HumanMessage(\"Hello, how are you?\")\n",
    "\n",
    "messages = [system_msg, human_msg]\n",
    "response = chat_groq.invoke(messages)  # It returns an AI_Message\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea61d3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='# The Road to Artificial General Intelligence  \\n*A researcher’s take on the promise, the pitfalls, and the next steps*\\n\\n---\\n\\n## 1.  What is AGI, and why does it matter?\\n\\nArtificial General Intelligence (AGI) is the idea of a machine that can *understand, learn, and apply knowledge* across *any* domain the way a human can. In practice, an AGI would:\\n\\n| Capability | Narrow AI example | AGI expectation |\\n|------------|------------------|-----------------|\\n| **Learning from few examples** | Few‑shot image classification | Learn a new skill after a single demonstration |\\n| **Transfer across domains** | A language model that writes code | A model that can write code, design circuits, and compose music with the same core system |\\n| **Self‑reflection & planning** | A chess engine | A system that can introspect, set long‑term goals, and adjust its strategies |\\n\\nWhy do we care? Because AGI would *reshape* every industry, from medicine to climate science, and it would also pose unprecedented safety, ethics, and governance challenges.\\n\\n---\\n\\n## 2.  The current landscape: Narrow AI vs. AGI\\n\\n### 2.1 The “tremendous” progress in narrow AI\\n\\n- **Large Language Models (LLMs)** – GPT‑4, Claude\\u202f3, Gemini – show remarkable language understanding and generation.  \\n- **Multimodal systems** – GPT‑4o, Gemini‑Pro, and LLaVA can process text, images, and audio.  \\n- **Self‑supervised learning** – Models trained on billions of tokens exhibit emergent reasoning abilities.  \\n\\nThese systems are *powerful* but *domain‑specific*. They excel within the confines of the data they were trained on and often lack true common‑sense reasoning or robust self‑reflection.\\n\\n### 2.2 The gulf to AGI\\n\\n| Feature | Present LLMs | AGI (hypothetical) |\\n|---------|--------------|--------------------|\\n| **Generalization** | Often brittle; fails on out‑of‑distribution inputs | Robust across domains |\\n| **Self‑monitoring** | No true introspection | Can introspect and correct itself |\\n| **Safety & alignment** | Narrow safety mitigations | Comprehensive alignment mechanisms |\\n| **Learning speed** | Requires massive data | Learns from few examples |\\n\\nThe “AI gap” is not just a technical hurdle; it’s also an *architectural* one. Current models are essentially pattern recognizers; they lack a unified representation of *cause*, *intent*, and *value*.\\n\\n---\\n\\n## 3.  Recent innovations that bring us closer\\n\\n| Innovation | What it brings | How it nudges AGI forward |\\n|------------|----------------|---------------------------|\\n| **Scaling laws** (OpenAI, DeepMind) | Predict how performance improves with data, compute, and parameters | Suggests that larger models may exhibit emergent AGI‑like behaviors |\\n| **Neuro‑symbolic hybrids** (e.g., GNN‑LLMs, AlphaFold‑style integration) | Combine neural perception with symbolic reasoning | Provides a pathway to explainable, goal‑driven AI |\\n| **Meta‑learning & few‑shot learning** (MAML, Reptile) | Models that adapt quickly | Moves toward “learning to learn” – a core AGI trait |\\n| **Multi‑agent coordination** (OpenAI’s DALL·E 3, GPT‑4o with agents) | Decentralized problem solving | Mimics human teamwork and division of cognitive labor |\\n| **Brain‑computer interfaces (BCIs)** (Neuralink, OpenBCI) | Direct neural signal decoding | Offers a bridge between biological and artificial cognition |\\n\\nThese innovations are *pieces of a puzzle*, not the final picture. However, they demonstrate that the *building blocks* for AGI are already in place—just need to be integrated and scaled responsibly.\\n\\n---\\n\\n## 4.  The hard problems that still block AGI\\n\\n### 4.1 Alignment & Value Learning\\n\\n- **Goal mis‑specification**: Current reward functions are proxies; they can be gamed.  \\n- **Robustness**: Even a well‑aligned system can diverge when faced with novel environments.  \\n- **Human‑in‑the‑loop**: We need scalable ways to keep humans informed without drowning in noise.\\n\\n### 4.2 Safety & Control\\n\\n- **Exploration vs. safety**: An AGI must explore to learn, but uncontrolled exploration can be dangerous.  \\n- **Fail‑safe mechanisms**: Hard‑wiring shutdowns or “kill switches” are insufficient without a deep understanding of the system’s internal state.  \\n- **Scalable oversight**: How do we audit an AGI that can generate new sub‑modules autonomously?\\n\\n### 4.3 Cognitive Architecture\\n\\n- **Unified representation**: Current systems use separate modules for vision, language, and planning. A true AGI would need a *single* coherent representation.  \\n- **Consciousness & self‑modeling**: Even if not required for functional AGI, a self‑model may be essential for introspection and alignment.\\n\\n### 4.4 Ethics & Governance\\n\\n- **Bias & fairness**: AGI will amplify existing biases unless actively mitigated.  \\n- **Equitable access**: Concentration of power in a few corporate labs risks exacerbating inequality.  \\n- **Policy frameworks**: International agreements, like the proposed “AGI Accord,” are still nascent.\\n\\n---\\n\\n## 5.  Emerging research directions\\n\\n| Direction | Key questions | Promising work |\\n|-----------|---------------|----------------|\\n| **Unified multimodal architectures** | Can we build a single network that handles text, vision, audio, and reasoning? | GPT‑4o, Gemini‑Pro, and emerging “universal transformers” |\\n| **Self‑supervised meta‑learning** | How to train a model that *learns* how to learn? | Meta‑learning frameworks, few‑shot LLMs |\\n| **Neuro‑symbolic integration** | How to marry the flexibility of neural nets with the precision of symbolic logic? | AlphaFold, Graph Neural Networks (GNNs) + LLMs |\\n| **Safety‑by‑design** | Can we embed safety constraints into the architecture itself? | Differentiable programming with constraints, adversarial training |\\n| **Open‑source AGI research** | How to democratize AGI progress? | EleutherAI, OpenAI’s open‑source models, open‑source alignment projects |\\n\\n---\\n\\n## 6.  The timeline debate\\n\\n- **Optimists** (e.g., Eliezer Yudkowsky, Nick Bostrom) predict AGI within **10–30 years**.  \\n- **Skeptics** (e.g., Stuart Russell, Tim Urban) argue **centuries** or that AGI is *unnecessary* given the power of narrow AI.  \\n\\nMost researchers agree: *We’ll see significant AGI‑like capabilities in the next decade, but a fully autonomous, human‑level AGI remains speculative.* The key is to build **robust, safe, and aligned systems** *along the way*.\\n\\n---\\n\\n## 7.  What researchers, practitioners, and policy makers can do now\\n\\n1. **Invest in interdisciplinary research** – bring together cognitive science, neuroscience, philosophy, and computer science.  \\n2. **Prioritize safety research** – fund projects that study alignment, interpretability, and robustness.  \\n3. **Encourage open collaboration** – share datasets, benchmarks, and safety protocols.  \\n4. **Develop governance frameworks** – create global standards for AGI safety and ethics.  \\n5. **Educate the public** – demystify AGI, explain risks, and foster informed debate.\\n\\n---\\n\\n## 8.  Conclusion: The AGI promise and the AGI peril\\n\\nArtificial General Intelligence sits at the *intersection of wonder and warning*. The recent advances in large models, multimodal learning, and neuro‑symbolic integration give us a glimpse of what might be possible. Yet the same progress exposes deep questions about alignment, safety, and human values that no amount of compute can solve on its own.\\n\\nIf we can channel the current momentum into *responsible, transparent, and collaborative* research, we may not only approach AGI but also ensure that it benefits humanity as a whole. The journey is long, the stakes high, but the potential rewards—solving climate change, eradicating disease, unlocking new frontiers of knowledge—are too great to ignore.\\n\\n*Let’s build AGI with care, not just capability.*', additional_kwargs={'reasoning_content': 'User wants a blog post on AGI. The developer says \"You are a researcher writing excellent blogs on recent innovations.\" So we should produce a high-quality blog post on AGI. The user says \"Write a blog post on AGI\". We should produce a well-structured blog post, maybe with sections, introduction, background, current state, challenges, future directions, conclusion. We should keep it engaging. Also keep in mind the instruction: \"You are a researcher writing excellent blogs on recent innovations.\" So the tone should be expert, but accessible. We should mention recent innovations, maybe GPT-4, GPT-5, Claude, etc. But AGI is a broader concept. We should mention the difference between narrow AI and AGI, the challenges, potential breakthroughs, safety, alignment, etc. Provide references or at least mention research directions. Use a friendly but scholarly tone. Also mention that AGI is still an aspirational goal.\\n\\nWe should produce a blog post that reads like a well-written article. Possibly include subheadings, bullet points, maybe a call to action. The user didn\\'t specify length, but we should aim for a moderate length, maybe 800-1200 words. We\\'ll produce a blog post with a title, introduction, sections, conclusion. We\\'ll mention recent innovations like GPT-4, GPT-4o, Claude 3, etc. We\\'ll mention that AGI remains elusive. We\\'ll mention the research community, the role of open-source, the need for interdisciplinary research. We\\'ll mention alignment, safety, and governance. We\\'ll mention possible future breakthroughs like neuromorphic computing, integrated brain-computer interfaces, etc. We\\'ll keep it accessible to a general audience but still research oriented.\\n\\nWe should also be careful not to misrepresent. We can mention that no one has proven AGI yet. We can mention that AGI is defined by a system that can perform any intellectual task that a human can. We\\'ll mention the Turing test, but also the limitations.\\n\\nWe should mention the concept of \"Artificial General Intelligence\" and the difference between \"Artificial Narrow Intelligence\" (ANI). We should mention the historical context: Turing, Minsky, etc. Also mention that current large language models are advanced but still narrow. We\\'ll talk about the \"AI alignment problem\", \"value alignment\", \"explainability\", etc.\\n\\nWe\\'ll talk about the role of emergent properties in large models, the possibility of scaling laws. We\\'ll mention the \"Scaling laws\" discovered by OpenAI and DeepMind. We\\'ll mention the \"AI safety\" community, the \"AGI safety\" research. We\\'ll mention the \"AI governance\" and \"AI policy\" aspects.\\n\\nWe should also mention that AGI might be realized through integration of multiple modalities, multi-agent systems, or by combining symbolic and neural approaches. We\\'ll mention \"Neuro-symbolic AI\", \"Hybrid models\".\\n\\nWe\\'ll mention that the timeline for AGI is uncertain, with estimates ranging from decades to centuries. We\\'ll mention that some researchers are optimistic (e.g., Yudkowsky, Bostrom), others are cautious.\\n\\nWe\\'ll also mention the \"AI alignment\" risk, \"superintelligence\" scenario. We\\'ll mention the \"AI safety\" community and the need for robust alignment research.\\n\\nWe should mention that there are also \"AI ethics\" issues, bias, fairness, privacy, etc.\\n\\nWe should provide a conclusion summarizing the state, the challenges, and the future directions.\\n\\nOk let\\'s produce.\\n\\n'}, response_metadata={'token_usage': {'completion_tokens': 2504, 'prompt_tokens': 103, 'total_tokens': 2607, 'completion_time': 2.590734313, 'prompt_time': 0.005029913, 'queue_time': 0.036442982, 'total_time': 2.595764226, 'completion_tokens_details': {'reasoning_tokens': 699}}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e189667b30', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--c52dea98-5f0a-4054-8137-05ead67fb290-0', usage_metadata={'input_tokens': 103, 'output_tokens': 2504, 'total_tokens': 2607})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Message prompts: pass in a list of messages to the model by providing a list of message objects.\n",
    "messages = [\n",
    "    SystemMessage(\"You are a researcher writing excelent blogs on recent innovations\"),\n",
    "    HumanMessage(\"Write a blog post on AGI\"),\n",
    "    AIMessage(\"AGI is known...\")\n",
    "]\n",
    "\n",
    "response = chat_groq.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34cda600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='# Artificial General Intelligence: The Dawn of a New Era\\n\\n**Published on 12\\u202fNov\\u202f2025 – by Dr. A.\\u202fR. Patel, Research Fellow, Institute for Cognitive Systems**\\n\\n---\\n\\n## 1. What Is AGI?\\n\\nArtificial General Intelligence (AGI) is the holy grail of artificial intelligence research: a system that can understand, learn, and apply knowledge across an unrestricted range of tasks—much like a human mind. Unlike today’s *narrow* AI (think image classifiers, recommendation engines, or voice assistants), which excel only in the specific problem they were trained on, AGI promises to be a *flexible*, *adaptable*, and *self‑improving* intellect.\\n\\n> **AGI vs. Narrow AI**  \\n> | Feature | Narrow AI | AGI |  \\n> |---------|-----------|-----|  \\n> | Task scope | Single, well‑defined | Any cognitive task |  \\n> | Learning | Supervised or reinforcement on fixed data | Continual, unsupervised, meta‑learning |  \\n> | Transfer | Limited (e.g., transfer learning) | Seamless cross‑domain |  \\n> | Adaptability | Hard‑coded pipelines | Self‑organizing architectures |  \\n\\n---\\n\\n## 2. Why Is AGI Still “Holy” in 2025?\\n\\nThe progress we’ve seen over the past decade—transformers, large language models (LLMs), and multimodal AI—has blurred the line between “narrow” and “general.” Yet AGI remains elusive for several reasons:\\n\\n1. **Scalability of Knowledge** – Human cognition operates on an immense, dynamic knowledge graph that is constantly updated. Current models can ingest billions of tokens, but they still lack the ability to *organize* this information into a coherent, lifelong memory.\\n\\n2. **Causal Reasoning** – Humans reason causally, not just statistically. AGI must infer *why* something happens, not just *what* it predicts.\\n\\n3. **Common‑Sense Understanding** – The world is full of implicit rules (e.g., gravity, social norms). LLMs can mimic these but often fail when faced with contradictory or novel scenarios.\\n\\n4. **Self‑Improvement & Safety** – An AGI that can modify its own architecture poses unprecedented safety challenges. We need robust verification, alignment, and governance frameworks.\\n\\n---\\n\\n## 3. Recent Milestones (2023‑2025)\\n\\n| Year | Innovation | Impact on AGI |\\n|------|------------|---------------|\\n| **2023** | *Meta‑Learning Transformers* (M2T) | Enabled models to adapt to new tasks with only a handful of examples. |\\n| **2024** | *Unified Multimodal Foundation Model* (UMFM) | Combined vision, audio, text, and sensor data into a single representation. |\\n| **2025** | *Self‑Supervised Causal Discovery* (SSCD) | First algorithm that learns causal graphs from raw data without labeled interventions. |\\n| **2025** | *Neuro‑Symbolic Hybrid Architecture* (NSA) | Integrated deep learning with symbolic reasoning, achieving state‑of‑the‑art performance on open‑ended problem solving. |\\n\\n### Highlight: The Self‑Supervised Causal Discovery (SSCD) Algorithm\\n\\nSSCD, introduced by the *Causal AI Lab* at MIT, uses a novel objective that rewards the model for correctly predicting counterfactuals. In a benchmark with 50 real‑world datasets, SSCD outperformed human experts in causal inference by 27\\u202f%. This is a critical step toward AGI because causal knowledge is the backbone of generalization and safety.\\n\\n---\\n\\n## 4. The Architecture of Tomorrow’s AGI\\n\\n### 4.1. The “Brain‑Inspired” Core\\n\\nMost AGI proposals now converge on a *hybrid* architecture:\\n\\n1. **Neural Substrate** – Large transformer‑based modules that act as flexible function approximators.\\n2. **Symbolic Layer** – A knowledge graph with explicit rules, constraints, and ontologies.\\n3. **Meta‑Learning Engine** – A controller that decides how to allocate resources, when to query the symbolic layer, and how to fine‑tune the neural modules on the fly.\\n4. **Safety & Alignment Module** – Continuous monitoring of the system’s goals and actions, with human‑in‑the‑loop oversight.\\n\\n### 4.2. Continual Learning & Lifelong Memory\\n\\nA hallmark of AGI is the ability to *retain* knowledge while *acquiring* new skills. Recent research on *elastic weight consolidation* (EWC) and *memory‑augmented neural networks* (MANNs) suggests that we can mitigate catastrophic forgetting. Combined with *experience replay* from simulated environments, an AGI can build a lifelong knowledge base without the need for retraining from scratch.\\n\\n---\\n\\n## 5. Ethical & Societal Considerations\\n\\n| Domain | Key Questions | Current Approaches |\\n|--------|---------------|--------------------|\\n| **Alignment** | How do we ensure AGI’s goals match human values? | Value‑learning frameworks, inverse reinforcement learning (IRL). |\\n| **Transparency** | Can we interpret AGI’s decisions? | Explainable AI (XAI) modules, symbolic audit trails. |\\n| **Fairness** | Will AGI perpetuate biases? | Bias‑mitigation during training, diverse data pipelines. |\\n| **Governance** | Who regulates AGI? | International treaties (e.g., the *AI Governance Accord*), open‑source oversight. |\\n\\nThe *AI Governance Accord* (signed in 2024) is a landmark agreement that sets minimum safety standards for AGI prototypes. It mandates that any AGI system must be auditable, capable of *self‑diagnosis*, and subject to *human‑override* protocols.\\n\\n---\\n\\n## 6. Roadmap to AGI (2026‑2030)\\n\\n| Year | Milestone | Rationale |\\n|------|-----------|-----------|\\n| **2026** | *Unified Causal‑Neural System* (UCNS) | Integrate SSCD with transformer backbones. |\\n| **2027** | *Human‑Aligned AGI Demo* | Showcase AGI solving open‑ended tasks (e.g., designing a sustainable city). |\\n| **2028** | *Self‑Improving Loop* | AGI autonomously refines its architecture while maintaining alignment. |\\n| **2029** | *Regulated Deployment* | Limited, supervised deployment in critical domains (healthcare, climate modeling). |\\n| **2030** | *AGI‑Enabled Society* | Widespread AGI integration into everyday life, under strict oversight. |\\n\\n---\\n\\n## 7. What We Still Don’t Know\\n\\n- **Consciousness & Subjective Experience** – Does AGI need a subjective state to function?  \\n- **Scalable Alignment** – How to guarantee alignment as the system grows?  \\n- **Robustness to Adversarial Inputs** – Can AGI resist manipulation at scale?  \\n\\n---\\n\\n## 8. Final Thoughts\\n\\nThe journey toward AGI is no longer a speculative dream; it is an active, interdisciplinary research frontier. The convergence of large‑scale learning, causal reasoning, symbolic representation, and rigorous safety protocols is reshaping our understanding of what it means to *think*.\\n\\nAs we stand on the cusp of this new era, the stakes are higher than ever. The potential benefits—solving climate change, eradicating disease, democratizing knowledge—are immense. The risks—misaligned goals, loss of control, socioeconomic disruption—are equally profound.\\n\\nThe responsibility lies with researchers, policymakers, and society at large to steer AGI’s development with caution, curiosity, and a commitment to human flourishing.\\n\\n---\\n\\n### References\\n\\n1. **M2T: Meta‑Learning Transformers** – *NeurIPS 2023*  \\n2. **UMFM: Unified Multimodal Foundation Model** – *ICLR 2024*  \\n3. **SSCD: Self‑Supervised Causal Discovery** – *Science Advances, 2025*  \\n4. **NSA: Neuro‑Symbolic Hybrid Architecture** – *Proceedings of the AAAI Conference on Artificial Intelligence, 2025*  \\n5. **AI Governance Accord** – *UNESCO, 2024*  \\n\\n---\\n\\n*If you enjoyed this post, subscribe for more deep dives into the cutting‑edge of AI research. For collaboration inquiries or to discuss AGI policy, feel free to reach out via the contact form.*', additional_kwargs={'reasoning_content': 'The user wants a blog post on AGI. The assistant responded partially with \"AGI is known...\" but incomplete. We need to produce a complete blog post. The user wants a blog post on AGI. The developer instruction says: \"You are a researcher writing excelent blogs on recent innovations\". So we should produce a well-structured blog post, with engaging tone, clear sections, maybe some headings, some recent innovations. Should be a blog style. Also we need to incorporate recent developments like GPT-4, LLMs, RLHF, etc. Also mention challenges, future directions, ethical concerns. Provide references. Should be about 1000-1500 words maybe. Let\\'s produce.'}, response_metadata={'token_usage': {'completion_tokens': 1922, 'prompt_tokens': 103, 'total_tokens': 2025, 'completion_time': 1.995261113, 'prompt_time': 0.005709944, 'queue_time': 0.036817639, 'total_time': 2.000971057, 'completion_tokens_details': {'reasoning_tokens': 143}}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_feb9b278f1', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--cfe1d272-6603-424e-b77c-5f9be46c9903-0', usage_metadata={'input_tokens': 103, 'output_tokens': 1922, 'total_tokens': 2025})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary format : specify messages directly in OpenAI chat completions format.\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a researcher writing excelent blogs on recent innovations\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a blog post on AGI\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"AGI is known...\"}\n",
    "]\n",
    "response = chat_groq.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ea2f17",
   "metadata": {},
   "source": [
    "### Chains without LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16078d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's a summary of the paragraph:\\n\\nLangChain is a tool that makes it easy to build AI-powered agents and applications using Large Language Models (LLMs). With just 10 lines of code, you can connect to various LLM providers like OpenAI, Anthropic, and Google. LangChain provides pre-built architecture and model integrations to help you get started quickly. It's recommended for building basic agents and apps. For more advanced needs that require customization, control, and low latency, use LangGraph, which LangChain is built on top of.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# A chain connects prompt + model + execution logic\n",
    "summary_prompt = PromptTemplate.from_template(\"Summarize this paragraph:\\n{text}\")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=summary_prompt,\n",
    "    llm=chat_ollama\n",
    ")\n",
    "\n",
    "# Types of chains LLMChain, ConversationalChain, SequentialChain etc.\n",
    "\n",
    "text_input = \"\"\"\n",
    "LangChain is the easiest way to start building agents and applications powered by LLMs.\n",
    "With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more.\n",
    "LangChain provides a pre-built agent architecture and model integrations to help you \n",
    "get started quickly and seamlessly incorporate LLMs into your agents and applications.\n",
    "We recommend you use LangChain if you want to quickly build agents and autonomous applications.\n",
    "Use LangGraph, our low-level agent orchestration framework and runtime, when you have more\n",
    "advanced needs that require a combination of deterministic and agentic workflows, heavy customization,\n",
    "and carefully controlled latency.\n",
    "LangChain agents are built on top of LangGraph in order to provide durable \n",
    "execution, streaming, human-in-the-loop, persistence, and more.\n",
    "You do not need to know LangGraph for basic LangChain agent usage.\n",
    "\"\"\"\n",
    "result_chain = llm_chain.run({\"text\": text_input})\n",
    "result_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2929a9aa",
   "metadata": {},
   "source": [
    "### LCEL — Modern, Pipe-based Composition\n",
    "- Multiple chain types are there in LCEL using Runnables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0137aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is the translation:\\n\\nLes modèles de langage humains sont en train de changer le développement de l\\'IA.\\n\\nNote: \"Large Language Models\" is a direct translation, as it\\'s a technical term that refers to a specific type of artificial intelligence model. If you wanted to make it more idiomatic French, you could translate it to something like \"Les modèles d\\'apprentissage automatique avancés sont en train de changer le développement de l\\'IA\". But in this case, I stuck with the literal translation.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# LCEL allows chaining with '|' operator (Prompt → Model → Parser)\n",
    "translate_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following text to French: {sentence}\"\n",
    ")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "lcel_chain = translate_prompt | chat_ollama | parser # This is how chaining happens using the pipe '|' operator. \n",
    "lcel_result = lcel_chain.invoke({\"sentence\": \"Large Language Models are changing AI development.\"})\n",
    "lcel_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19240664",
   "metadata": {},
   "source": [
    "- Messages — help structure role-based context.\n",
    "- ChatModels — enable conversational LLMs with message awareness.\n",
    "- Chains (without LCEL) — explicit setup, more control, more boilerplate.\n",
    "- LCEL — concise, readable, and easier to debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df6a110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
