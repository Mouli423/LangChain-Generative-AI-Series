{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c275df15",
   "metadata": {},
   "source": [
    "# LangChain Text Splitters ‚Äì A Hands-On Guide\n",
    "\n",
    "This repository demonstrates how **LangChain Text Splitters** are used to divide large documents into smaller, meaningful chunks for better embeddings and retrieval in **Generative AI pipelines**.\n",
    "\n",
    "## Why Text Splitting?\n",
    "\n",
    "LLMs (like GPT-5, Groq, Gemini or Claude) have input size limits (token windows). Large documents can exceed those limits, causing:\n",
    "- Lost context or truncated responses  \n",
    "- Poor embedding quality  \n",
    "- Slow and inefficient retrieval  \n",
    "\n",
    "Text splitters solve this by **dividing text into overlapping, context-preserving chunks** before embedding or retrieval.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2844ab54",
   "metadata": {},
   "source": [
    "# 1. TokenTextSplitter \n",
    "### It splits the data by token count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b3584a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './speech.txt'}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content=' those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\n‚Ä¶\\n\\nIt will be all the easier for'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content=' of right and of fair play we profess to be fighting for.\\n\\n‚Ä¶\\n\\nIt will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content=' an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us‚Äîhowever hard it may be for them, for the time being, to believe that this is spoken from our hearts.\\n\\nWe have borne with their present government through all these bitter months because of that friendship‚Äîex'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content=' spoken from our hearts.\\n\\nWe have borne with their present government through all these bitter months because of that friendship‚Äîexercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content=' and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content=' may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\\n\\nIt is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content=' the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our hearts‚Äîfor democracy, for the right of those who submit to authority'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content=' fight for the things which we have always carried nearest our hearts‚Äîfor democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\\n\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "loader = TextLoader(\"./speech.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=25)\n",
    "\n",
    "texts = text_splitter.split_documents(docs)\n",
    "texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5543a057",
   "metadata": {},
   "source": [
    "# 2. CharacterTextSplitter - The text is split based on the number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf8ac5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 470, which is longer than the specified 75\n",
      "Created a chunk of size 347, which is longer than the specified 75\n",
      "Created a chunk of size 668, which is longer than the specified 75\n",
      "Created a chunk of size 982, which is longer than the specified 75\n",
      "Created a chunk of size 789, which is longer than the specified 75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './speech.txt'}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='Just because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='‚Ä¶'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='It will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us‚Äîhowever hard it may be for them, for the time being, to believe that this is spoken from our hearts.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='We have borne with their present government through all these bitter months because of that friendship‚Äîexercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='It is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our hearts‚Äîfor democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='To such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter=CharacterTextSplitter(\n",
    "    separator='\\n',\n",
    "    chunk_size=75,\n",
    "    chunk_overlap=15\n",
    ")\n",
    "\n",
    "texts=text_splitter.split_documents(docs)\n",
    "\n",
    "\"\"\"Text will be split only at new lines since we are using the new line (‚Äú\\n‚Äù) as the separator.\n",
    " If any chunk has a size more than 75 but no new lines in it, it will be returned as such.\"\"\"\n",
    "\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58194dac",
   "metadata": {},
   "source": [
    "# 3. RecursiveCharacterTextSplitter\n",
    "\n",
    "### This method uses multiple separators recursively to split the data until the chunk reaches the less than the chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf7c0bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './speech.txt'}, page_content='The world must be made safe for democracy.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='Its peace must be planted upon the tested foundations of political liberty.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='We have no selfish ends to serve.(?<=[.,])\\\\s+We desire no conquest,(?<=[.,])\\\\s+no dominion.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='no dominion.(?<=[.,])\\\\s+We seek no indemnities for ourselves,'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='no material compensation for the sacrifices we shall freely make.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='We are but one of the champions of the rights of mankind.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='Just because we fight without rancor and without selfish object,'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='seeking nothing for ourselves but what we shall wish to share with all free peoples,'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='we shall,(?<=[.,])\\\\s+I feel confident,'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='‚Ä¶'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='It will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus,'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='not in enmity toward a people or with the desire to bring any injury or disadvantage upon them,'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='We are,(?<=[.,])\\\\s+let me say again,(?<=[.,])\\\\s+the sincere friends of the German people,'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us‚Äîhowever hard it may be for them,'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='for the time being,(?<=[.,])\\\\s+to believe that this is spoken from our hearts.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='We have borne with their present government through all these bitter months because of that friendship‚Äîexercising a patience and forbearance which would otherwise have been impossible.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='We shall,(?<=[.,])\\\\s+happily,'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life,'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='They are,(?<=[.,])\\\\s+most of them,'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='as true and loyal Americans as if they had never known any other fealty or allegiance.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='If there should be disloyalty,'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='it will be dealt with with a firm hand of stern repression; but,'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='if it lifts its head at all,'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='it will lift it only here and there and without countenance except from a lawless and malignant few.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='It is a distressing and oppressive duty,(?<=[.,])\\\\s+gentlemen of the Congress,'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='which I have performed in thus addressing you.(?<=[.,])\\\\s+There are,(?<=[.,])\\\\s+it may be,'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='it may be,(?<=[.,])\\\\s+many months of fiery trial and sacrifice ahead of us.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='It is a fearful thing to lead this great peaceful people into war,'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='into the most terrible and disastrous of all wars,'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='civilization itself seeming to be in the balance.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='But the right is more precious than peace,'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='and we shall fight for the things which we have always carried nearest our hearts‚Äîfor democracy,'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='for the right of those who submit to authority to have a voice in their own governments,'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='for the rights and liberties of small nations,'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='To such a task we can dedicate our lives and our fortunes,'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='everything that we are and everything that we have,'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='God helping her,(?<=[.,])\\\\s+she can do no other.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    separators=['\\n\\n',r\"(?<=[.,])\\s+\"],\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    is_separator_regex=True,\n",
    "    keep_separator=False\n",
    ")\n",
    "\n",
    "texts=text_splitter.split_documents(docs)\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4be306",
   "metadata": {},
   "source": [
    "# 4. MarkdownHeaderTextSplitter \n",
    "### It is used when the text follows a structured Markdown format ‚Äî  \n",
    "### like articles, documentation, or hierarchical notes (with `#`, `##`, `###` headers, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac7893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'LangChain TextSplitters'}, page_content='Text Splitters are most important elements the langchain pipeline which divides the data in meaningful chunks.'),\n",
       " Document(metadata={'Header 1': 'LangChain TextSplitters', 'Header 2': 'Types of TextSplitters'}, page_content='1.  Length-Based\\n2. Text Structured Based\\n3. Document Structured Based\\n4. Semantic Meaning Based'),\n",
       " Document(metadata={'Header 1': 'LangChain TextSplitters', 'Header 2': 'Types of TextSplitters', 'Header 3': 'Reasons to Splt documents'}, page_content='1. Handling non-uniform document lengths\\n2. Overcoming model limitations\\n3. Improving representaion quality\\n4. Optimizing Computational Resources')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "markdown_text = \"\"\"\n",
    "# LangChain TextSplitters\n",
    "Text Splitters are most important elements the langchain pipeline which divides the data in meaningful chunks.\n",
    "## Types of TextSplitters\n",
    "1.  Length-Based\n",
    "2. Text Structured Based\n",
    "3. Document Structured Based\n",
    "4. Semantic Meaning Based\n",
    "### Reasons to Splt documents\n",
    "1. Handling non-uniform document lengths\n",
    "2. Overcoming model limitations\n",
    "3. Improving representaion quality\n",
    "4. Optimizing Computational Resources\n",
    "\"\"\"\n",
    "\n",
    "splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=[(\"#\", \"Header 1\"), (\"##\", \"Header 2\"),('###','Header 3')]\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(markdown_text)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8697c65f",
   "metadata": {},
   "source": [
    "# 5. HTMLHeaderTextSplitter\n",
    "### It is a \"structure-aware\" text splitter that splits text at the HTML element level and adds metadata for each header \"relevant\" to any given chunk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a2a103",
   "metadata": {},
   "source": [
    "Choosing the Right Splitter\n",
    "- HTMLHeaderTextSplitter:When You need to split an HTML document based on its header hierarchy and maintain metadata about the headers.\n",
    "- Use HTMLSectionSplitter when: You need to split the document into larger, more general sections, possibly based on custom tags or font sizes.\n",
    "- Use HTMLSemanticPreservingSplitter when: You need to split the document into chunks while preserving semantic elements like tables and lists, ensuring that they are not split and that their context is maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee57736f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='!function(){function t(t){document.documentElement.setAttribute(\"data-theme\",t)}var e=function(){try{return new URLSearchParams(window.location.search).get(\"docusaurus-theme\")}catch(t){}}()||function(){try{return window.localStorage.getItem(\"theme\")}catch(t){}}();null!==e?t(e):window.matchMedia(\"(prefers-color-scheme: dark)\").matches?t(\"dark\"):(window.matchMedia(\"(prefers-color-scheme: light)\").matches,t(\"light\"))}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith(\"docusaurus-data-\")){var a=t.replace(\"docusaurus-data-\",\"data-\");document.documentElement.setAttribute(a,e)}}catch(t){}}(),document.documentElement.setAttribute(\"data-announcement-bar-initially-dismissed\",function(){try{return\"true\"===localStorage.getItem(\"docusaurus.announcement.dismiss\")}catch(t){}return!1}())  \\nSkip to main content  \\n‚ö†Ô∏è THESE DOCS ARE OUTDATED.  \\nVisit the new v1.0 docs  \\nIntegrations  \\nAPI Reference  \\nMore  \\nContributing  \\nPeople  \\nError reference  \\nLangSmith  \\nLangGraph  \\nLangChain Hub  \\nLangChain JS/TS  \\nv0.3  \\nv0.3  \\nv0.2  \\nv0.1  \\nüí¨  \\nSearch  \\nIntroduction  \\nTutorials  \\nBuild a Question Answering application over a Graph Database  \\nindex  \\nBuild a simple LLM application with chat models and prompt templates  \\nBuild a Chatbot  \\nBuild a Retrieval Augmented Generation (RAG) App: Part 2  \\nBuild an Extraction Chain  \\nBuild an Agent  \\nTagging  \\nBuild a Retrieval Augmented Generation (RAG) App: Part 1  \\nBuild a semantic search engine  \\nBuild a Question/Answering system over SQL data  \\nSummarize Text  \\nHow-to guides  \\nindex  \\nHow to use tools in a chain  \\nHow to use a vectorstore as a retriever  \\nHow to add memory to chatbots  \\nHow to use example selectors  \\nHow to add a semantic layer over graph database  \\nHow to invoke runnables in parallel  \\nHow to stream chat model responses  \\nHow to add default invocation args to a Runnable  \\nHow to add retrieval to chatbots  \\nHow to use few shot examples in chat models  \\nHow to do tool/function calling  \\nHow to install LangChain packages  \\nHow to add examples to the prompt for query analysis  \\nHow to use few shot examples  \\nHow to run custom functions  \\nHow to use output parsers to parse an LLM response into structured format  \\nHow to handle cases where no queries are generated  \\nHow to route between sub-chains  \\nHow to return structured data from a model  \\nHow to summarize text through parallelization  \\nHow to summarize text through iterative refinement  \\nHow to summarize text in a single LLM call  \\nHow to use toolkits  \\nHow to add ad-hoc tool calling capability to LLMs and Chat Models  \\nBuild an Agent with AgentExecutor (Legacy)  \\nHow to construct knowledge graphs  \\nHow to partially format prompt templates  \\nHow to handle multiple queries when doing query analysis  \\nHow to use built-in tools and toolkits  \\nHow to pass through arguments from one step to the next  \\nHow to compose prompts together  \\nHow to handle multiple retrievers when doing query analysis  \\nHow to add values to a chain\\'s state  \\nHow to construct filters for query analysis  \\nHow to configure runtime chain internals  \\nHow to deal with high-cardinality categoricals when doing query analysis  \\nCustom Document Loader  \\nHow to use the MultiQueryRetriever  \\nHow to add scores to retriever results  \\nCaching  \\nHow to use callbacks in async environments  \\nHow to attach callbacks to a runnable  \\nHow to propagate callbacks  constructor  \\nHow to dispatch custom callback events  \\nHow to pass callbacks in at runtime  \\nHow to split by character  \\nHow to cache chat model responses  \\nHow to handle rate limits  \\nHow to init any model in one line  \\nHow to track token usage in ChatModels  \\nHow to add tools to chatbots  \\nHow to split code  \\nHow to do retrieval with contextual compression  \\nHow to convert Runnables to Tools  \\nHow to create custom callback handlers  \\nHow to create a custom chat model class  \\nCustom Embeddings  \\nHow to create a custom LLM class  \\nCustom Retriever  \\nHow to create tools  \\nHow to debug your LLM apps  \\nHow to load CSVs  \\nHow to load documents from a directory  \\nHow to load HTML  \\nHow to load JSON  \\nHow to load Markdown  \\nHow to load Microsoft Office files  \\nHow to load PDFs  \\nHow to load web pages  \\nHow to create a dynamic (self-constructing) chain  \\nText embedding models  \\nHow to combine results from multiple retrievers  \\nHow to select examples from a LangSmith dataset  \\nHow to select examples by length  \\nHow to select examples by maximal marginal relevance (MMR)  \\nHow to select examples by n-gram overlap  \\nHow to select examples by similarity  \\nHow to use reference examples when doing extraction  \\nHow to handle long text when doing extraction  \\nHow to use prompting alone (no tool calling) to do extraction  \\nHow to add fallbacks to a runnable  \\nHow to filter messages  \\nHybrid Search  \\nHow to use the LangChain indexing API  \\nHow to inspect runnables  \\nLangChain Expression Language Cheatsheet  \\nHow to cache LLM responses  \\nHow to track token usage for LLMs  \\nRun models locally  \\nHow to get log probabilities  \\nHow to reorder retrieved results to mitigate the \"lost in the middle\" effect  \\nHow to split Markdown by Headers  \\nHow to merge consecutive messages of the same type  \\nHow to add message history  \\nHow to migrate from legacy LangChain agents to LangGraph  \\nHow to retrieve using multiple vectors per document  \\nHow to pass multimodal data to models  \\nHow to use multimodal prompts  \\nHow to create a custom Output Parser  \\nHow to use the output-fixing parser  \\nHow to parse JSON output  \\nHow to retry when a parsing error occurs  \\nHow to parse text from message objects  \\nHow to parse XML output  \\nHow to parse YAML output  \\nHow to use the Parent Document Retriever  \\nHow to use LangChain with different Pydantic versions  \\nHow to add chat history  \\nHow to get a RAG application to add citations  \\nHow to do per-user retrieval  \\nHow to get your RAG application to return sources  \\nHow to stream results from your RAG application  \\nHow to split JSON data  \\nHow to recursively split text by characters  \\nResponse metadata  \\nHow to pass runtime secrets to runnables  \\nHow to do \"self-querying\" retrieval  \\nHow to split text based on semantic similarity  \\nHow to chain runnables  \\nHow to save and load LangChain objects  \\nHow to split text by tokens  \\nHow to split HTML  \\nHow to do question answering over CSVs  \\nHow to deal with large databases when doing SQL question-answering  \\nHow to better prompt when doing SQL question-answering  \\nHow to do query validation as part of SQL question-answering  \\nHow to stream runnables  \\nHow to stream responses from an LLM  \\nHow to use a time-weighted vector store retriever  \\nHow to return artifacts from a tool  \\nHow to use chat models to call tools  \\nHow to disable parallel tool calling  \\nHow to force models to call a tool  \\nHow to access the RunnableConfig from a tool  \\nHow to pass tool outputs to chat models  \\nHow to pass run time values to tools  \\nHow to stream events from a tool  \\nHow to stream tool calls  \\nHow to convert tools to OpenAI Functions  \\nHow to handle tool errors  \\nHow to use few-shot prompting with tool calling  \\nHow to add a human-in-the-loop for tools  \\nHow to bind model-specific tools  \\nHow to trim messages  \\nHow to create and query vector stores  \\nConceptual guide  \\nAgents  \\nArchitecture  \\nAsync programming with LangChain  \\nCallbacks  \\nChat history  \\nChat models  \\nDocument loaders  \\nEmbedding models  \\nEvaluation  \\nExample selectors  \\nFew-shot prompting  \\nindex  \\nKey-value stores  \\nLangChain Expression Language (LCEL)  \\nMessages  \\nMultimodality  \\nOutput parsers  \\nPrompt Templates  \\nRetrieval augmented generation (RAG)  \\nRetrieval  \\nRetrievers  \\nRunnable interface  \\nStreaming  \\nStructured outputs  \\nTesting  \\nString-in, string-out llms  \\nText splitters  \\nTokens  \\nTool calling  \\nTools  \\nTracing  \\nVector stores  \\nWhy LangChain?  \\nEcosystem  \\nü¶úüõ†Ô∏è LangSmith  \\nü¶úüï∏Ô∏è LangGraph  \\nVersions  \\nv0.3  \\nv0.2  \\nPydantic compatibility  \\nMigrating from v0.0 chains  \\nHow to migrate from v0.0 chains  \\nMigrating from ConstitutionalChain  \\nMigrating from ConversationalChain  \\nMigrating from ConversationalRetrievalChain  \\nMigrating from LLMChain  \\nMigrating from LLMMathChain  \\nMigrating from LLMRouterChain  \\nMigrating from MapReduceDocumentsChain  \\nMigrating from MapRerankDocumentsChain  \\nMigrating from MultiPromptChain  \\nMigrating from RefineDocumentsChain  \\nMigrating from RetrievalQA  \\nMigrating from StuffDocumentsChain  \\nUpgrading to LangGraph memory  \\nHow to migrate to LangGraph memory  \\nHow to use BaseChatMessageHistory with LangGraph  \\nMigrating off ConversationBufferMemory or ConversationStringBufferMemory  \\nMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory  \\nMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemory  \\nA Long-Term Memory Agent  \\nRelease policy  \\nSecurity Policy  \\nHow-to guides  \\nHow to split HTML  \\nOn this page'),\n",
       " Document(metadata={'Header 1': 'How to split HTML'}, page_content='How to split HTML'),\n",
       " Document(metadata={}, page_content='Splitting HTML documents into manageable chunks is essential for various text processing tasks such as natural language processing, search indexing, and more. In this guide, we will explore three different text splitters provided by LangChain that you can use to split HTML content effectively:  \\nHTMLHeaderTextSplitter  \\nHTMLSectionSplitter  \\nHTMLSemanticPreservingSplitter  \\nEach of these splitters has unique features and use cases. This guide will help you understand the differences between them, why you might choose one over the others, and how to use them effectively.  \\n%pip install -qU langchain-text-splitters'),\n",
       " Document(metadata={'Header 2': 'Overview of the Splitters'}, page_content='Overview of the Splitters'),\n",
       " Document(metadata={'Header 2': 'Overview of the Splitters'}, page_content='\\u200b  \\nHTMLHeaderTextSplitter  \\n\\u200b  \\ninfo  \\nUseful when you want to preserve the hierarchical structure of a document based on its headings.  \\n: Splits HTML text based on header tags (e.g., , , , etc.), and adds metadata for each header relevant to any given chunk.  \\nDescription  \\n<h1>  \\n<h2>  \\n<h3>  \\n:  \\nCapabilities  \\nSplits text at the HTML element level.  \\nPreserves context-rich information encoded in document structures.  \\nCan return chunks element by element or combine elements with the same metadata.  \\nHTMLSectionSplitter  \\n\\u200b  \\ninfo  \\nUseful when you want to split HTML documents into larger sections, such as , , or custom-defined sections.  \\n<section>  \\n<div>  \\n: Similar to HTMLHeaderTextSplitter but focuses on splitting HTML into sections based on specified tags.  \\nDescription  \\n:  \\nCapabilities  \\nUses XSLT transformations to detect and split sections.  \\nInternally uses for large sections.  \\nRecursiveCharacterTextSplitter  \\nConsiders font sizes to determine sections.  \\nHTMLSemanticPreservingSplitter  \\n\\u200b  \\ninfo  \\nIdeal when you need to ensure that structured elements are not split across chunks, preserving contextual relevancy.  \\n: Splits HTML content into manageable chunks while preserving the semantic structure of important elements like tables, lists, and other HTML components.  \\nDescription  \\n:  \\nCapabilities  \\nPreserves tables, lists, and other specified HTML elements.  \\nAllows custom handlers for specific HTML tags.  \\nEnsures that the semantic meaning of the document is maintained.  \\nBuilt in normalization & stopword removal'),\n",
       " Document(metadata={'Header 2': 'Overview of the Splitters', 'Header 3': 'Choosing the Right Splitter'}, page_content='Choosing the Right Splitter'),\n",
       " Document(metadata={'Header 2': 'Overview of the Splitters', 'Header 3': 'Choosing the Right Splitter'}, page_content='\\u200b  \\n: You need to split an HTML document based on its header hierarchy and maintain metadata about the headers.  \\nUse when  \\nHTMLHeaderTextSplitter  \\n: You need to split the document into larger, more general sections, possibly based on custom tags or font sizes.  \\nUse when  \\nHTMLSectionSplitter  \\n: You need to split the document into chunks while preserving semantic elements like tables and lists, ensuring that they are not split and that their context is maintained.  \\nUse when  \\nHTMLSemanticPreservingSplitter  \\nFeature  \\nHTMLHeaderTextSplitter  \\nHTMLSectionSplitter  \\nHTMLSemanticPreservingSplitter  \\nSplits based on headers  \\nYes  \\nYes  \\nYes  \\nPreserves semantic elements (tables, lists)  \\nNo  \\nNo  \\nYes  \\nAdds metadata for headers  \\nYes  \\nYes  \\nYes  \\nCustom handlers for HTML tags  \\nNo  \\nNo  \\nYes  \\nPreserves media (images, videos)  \\nNo  \\nNo  \\nYes  \\nConsiders font sizes  \\nNo  \\nYes  \\nNo  \\nUses XSLT transformations  \\nNo  \\nYes  \\nNo'),\n",
       " Document(metadata={'Header 2': 'Example HTML Document'}, page_content='Example HTML Document'),\n",
       " Document(metadata={'Header 2': 'Example HTML Document'}, page_content='\\u200b  \\nLet\\'s use the following HTML document as an example:  \\nhtml_string  \\n=  \\n\"\"\"  \\n<!DOCTYPE html>  \\n<html lang=\\'en\\'>  \\n<head>  \\n<meta charset=\\'UTF-8\\'>  \\n<meta name=\\'viewport\\' content=\\'width=device-width, initial-scale=1.0\\'>  \\n<title>Fancy Example HTML Page</title>  \\n</head>  \\n<body>  \\n<h1>Main Title</h1>  \\n<p>This is an introductory paragraph with some basic content.</p>  \\n<h2>Section 1: Introduction</h2>  \\n<p>This section introduces the topic. Below is a list:</p>  \\n<ul>  \\n<li>First item</li>  \\n<li>Second item</li>  \\n<li>Third item with <strong>bold text</strong> and <a href=\\'#\\'>a link</a></li>  \\n</ul>  \\n<h3>Subsection 1.1: Details</h3>  \\n<p>This subsection provides additional details. Here\\'s a table:</p>  \\n<table border=\\'1\\'>  \\n<thead>  \\n<tr>  \\n<th>Header 1</th>  \\n<th>Header 2</th>  \\n<th>Header 3</th>  \\n</tr>  \\n</thead>  \\n<tbody>  \\n<tr>  \\n<td>Row 1, Cell 1</td>  \\n<td>Row 1, Cell 2</td>  \\n<td>Row 1, Cell 3</td>  \\n</tr>  \\n<tr>  \\n<td>Row 2, Cell 1</td>  \\n<td>Row 2, Cell 2</td>  \\n<td>Row 2, Cell 3</td>  \\n</tr>  \\n</tbody>  \\n</table>  \\n<h2>Section 2: Media Content</h2>  \\n<p>This section contains an image and a video:</p>  \\n<img src=\\'example_image_link.mp4\\' alt=\\'Example Image\\'>  \\n<video controls width=\\'250\\' src=\\'example_video_link.mp4\\' type=\\'video/mp4\\'>  \\nYour browser does not support the video tag.  \\n</video>  \\n<h2>Section 3: Code Example</h2>  \\n<p>This section contains a code block:</p>  \\n<pre><code data-lang=\"html\">  \\n&lt;div&gt;  \\n&lt;p&gt;This is a paragraph inside a div.&lt;/p&gt;  \\n&lt;/div&gt;  \\n</code></pre>  \\n<h2>Conclusion</h2>  \\n<p>This is the conclusion of the document.</p>  \\n</body>  \\n</html>  \\n\"\"\"'),\n",
       " Document(metadata={'Header 2': 'Using HTMLHeaderTextSplitter'}, page_content='Using HTMLHeaderTextSplitter'),\n",
       " Document(metadata={'Header 2': 'Using HTMLHeaderTextSplitter'}, page_content='\\u200b  \\nis a \"structure-aware\" that splits text at the HTML element level and adds metadata for each header \"relevant\" to any given chunk. It can return chunks element by element or combine elements with the same metadata, with the objectives of (a) keeping related text grouped (more or less) semantically and (b) preserving context-rich information encoded in document structures. It can be used with other text splitters as part of a chunking pipeline.  \\nHTMLHeaderTextSplitter  \\ntext splitter  \\nIt is analogous to the for markdown files.  \\nMarkdownHeaderTextSplitter  \\nTo specify what headers to split on, specify when instantiating as shown below.  \\nheaders_to_split_on  \\nHTMLHeaderTextSplitter  \\nfrom  \\nlangchain_text_splitters  \\nimport  \\nHTMLHeaderTextSplitter  \\nheaders_to_split_on  \\n=  \\n[  \\n(  \\n\"h1\"  \\n,  \\n\"Header 1\"  \\n)  \\n,  \\n(  \\n\"h2\"  \\n,  \\n\"Header 2\"  \\n)  \\n,  \\n(  \\n\"h3\"  \\n,  \\n\"Header 3\"  \\n)  \\n,  \\n]  \\nhtml_splitter  \\n=  \\nHTMLHeaderTextSplitter  \\n(  \\nheaders_to_split_on  \\n)  \\nhtml_header_splits  \\n=  \\nhtml_splitter  \\n.  \\nsplit_text  \\n(  \\nhtml_string  \\n)  \\nhtml_header_splits  \\n[Document(metadata={\\'Header 1\\': \\'Main Title\\'}, page_content=\\'This is an introductory paragraph with some basic content.\\'),  \\nDocument(metadata={\\'Header 1\\': \\'Main Title\\', \\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\\'This section introduces the topic. Below is a list:  \\\\nFirst item Second item Third item with bold text and a link\\'),  \\nDocument(metadata={\\'Header 1\\': \\'Main Title\\', \\'Header 2\\': \\'Section 1: Introduction\\', \\'Header 3\\': \\'Subsection 1.1: Details\\'}, page_content=\"This subsection provides additional details. Here\\'s a table:\"),  \\nDocument(metadata={\\'Header 1\\': \\'Main Title\\', \\'Header 2\\': \\'Section 2: Media Content\\'}, page_content=\\'This section contains an image and a video:\\'),  \\nDocument(metadata={\\'Header 1\\': \\'Main Title\\', \\'Header 2\\': \\'Section 3: Code Example\\'}, page_content=\\'This section contains a code block:\\'),  \\nDocument(metadata={\\'Header 1\\': \\'Main Title\\', \\'Header 2\\': \\'Conclusion\\'}, page_content=\\'This is the conclusion of the document.\\')]  \\nTo return each element together with their associated headers, specify when instantiating :  \\nreturn_each_element=True  \\nHTMLHeaderTextSplitter  \\nhtml_splitter  \\n=  \\nHTMLHeaderTextSplitter  \\n(  \\nheaders_to_split_on  \\n,  \\nreturn_each_element  \\n=  \\nTrue  \\n,  \\n)  \\nhtml_header_splits_elements  \\n=  \\nhtml_splitter  \\n.  \\nsplit_text  \\n(  \\nhtml_string  \\n)  \\nComparing with the above, where elements are aggregated by their headers:  \\nfor  \\nelement  \\nin  \\nhtml_header_splits  \\n[  \\n:  \\n2  \\n]  \\n:  \\nprint  \\n(  \\nelement  \\n)  \\npage_content=\\'This is an introductory paragraph with some basic content.\\' metadata={\\'Header 1\\': \\'Main Title\\'}  \\npage_content=\\'This section introduces the topic. Below is a list:  \\nFirst item Second item Third item with bold text and a link\\' metadata={\\'Header 1\\': \\'Main Title\\', \\'Header 2\\': \\'Section 1: Introduction\\'}  \\nNow each element is returned as a distinct :  \\nDocument  \\nfor  \\nelement  \\nin  \\nhtml_header_splits_elements  \\n[  \\n:  \\n3  \\n]  \\n:  \\nprint  \\n(  \\nelement  \\n)  \\npage_content=\\'This is an introductory paragraph with some basic content.\\' metadata={\\'Header 1\\': \\'Main Title\\'}  \\npage_content=\\'This section introduces the topic. Below is a list:\\' metadata={\\'Header 1\\': \\'Main Title\\', \\'Header 2\\': \\'Section 1: Introduction\\'}  \\npage_content=\\'First item Second item Third item with bold text and a link\\' metadata={\\'Header 1\\': \\'Main Title\\', \\'Header 2\\': \\'Section 1: Introduction\\'}'),\n",
       " Document(metadata={'Header 2': 'Using HTMLHeaderTextSplitter', 'Header 3': 'How to split from a URL or HTML file:'}, page_content='How to split from a URL or HTML file:'),\n",
       " Document(metadata={'Header 2': 'Using HTMLHeaderTextSplitter', 'Header 3': 'How to split from a URL or HTML file:'}, page_content='\\u200b  \\nTo read directly from a URL, pass the URL string into the method.  \\nsplit_text_from_url  \\nSimilarly, a local HTML file can be passed to the method.  \\nsplit_text_from_file  \\nurl  \\n=  \\n\"https://plato.stanford.edu/entries/goedel/\"  \\nheaders_to_split_on  \\n=  \\n[  \\n(  \\n\"h1\"  \\n,  \\n\"Header 1\"  \\n)  \\n,  \\n(  \\n\"h2\"  \\n,  \\n\"Header 2\"  \\n)  \\n,  \\n(  \\n\"h3\"  \\n,  \\n\"Header 3\"  \\n)  \\n,  \\n(  \\n\"h4\"  \\n,  \\n\"Header 4\"  \\n)  \\n,  \\n]  \\nhtml_splitter  \\n=  \\nHTMLHeaderTextSplitter  \\n(  \\nheaders_to_split_on  \\n)  \\n# for local file use html_splitter.split_text_from_file(<path_to_file>)  \\nhtml_header_splits  \\n=  \\nhtml_splitter  \\n.  \\nsplit_text_from_url  \\n(  \\nurl  \\n)'),\n",
       " Document(metadata={'Header 2': 'Using HTMLHeaderTextSplitter', 'Header 3': 'How to constrain chunk sizes:'}, page_content='How to constrain chunk sizes:'),\n",
       " Document(metadata={'Header 2': 'Using HTMLHeaderTextSplitter', 'Header 3': 'How to constrain chunk sizes:'}, page_content=\"\\u200b  \\n, which splits based on HTML headers, can be composed with another splitter which constrains splits based on character lengths, such as .  \\nHTMLHeaderTextSplitter  \\nRecursiveCharacterTextSplitter  \\nThis can be done using the method of the second splitter:  \\n.split_documents  \\nfrom  \\nlangchain_text_splitters  \\nimport  \\nRecursiveCharacterTextSplitter  \\nchunk_size  \\n=  \\n500  \\nchunk_overlap  \\n=  \\n30  \\ntext_splitter  \\n=  \\nRecursiveCharacterTextSplitter  \\n(  \\nchunk_size  \\n=  \\nchunk_size  \\n,  \\nchunk_overlap  \\n=  \\nchunk_overlap  \\n)  \\n# Split  \\nsplits  \\n=  \\ntext_splitter  \\n.  \\nsplit_documents  \\n(  \\nhtml_header_splits  \\n)  \\nsplits  \\n[  \\n80  \\n:  \\n85  \\n]  \\n[Document(metadata={'Header 1': 'Kurt G√∂del', 'Header 2': '2. G√∂del‚Äôs Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='We see that G√∂del first tried to reduce the consistency problem for analysis to that of arithmetic. This seemed to require a truth definition for arithmetic, which in turn led to paradoxes, such as the Liar paradox (‚ÄúThis sentence is false‚Äù) and Berry‚Äôs paradox (‚ÄúThe least number not defined by an expression consisting of just fourteen English words‚Äù). G√∂del then noticed that such paradoxes would not necessarily arise if truth were replaced by provability. But this means that arithmetic truth'),  \\nDocument(metadata={'Header 1': 'Kurt G√∂del', 'Header 2': '2. G√∂del‚Äôs Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='means that arithmetic truth and arithmetic provability are not co-extensive ‚Äî whence the First Incompleteness Theorem.'),  \\nDocument(metadata={'Header 1': 'Kurt G√∂del', 'Header 2': '2. G√∂del‚Äôs Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='This account of G√∂del‚Äôs discovery was told to Hao Wang very much after the fact; but in G√∂del‚Äôs contemporary correspondence with Bernays and Zermelo, essentially the same description of his path to the theorems is given. (See G√∂del 2003a and G√∂del 2003b respectively.) From those accounts we see that the undefinability of truth in arithmetic, a result credited to Tarski, was likely obtained in some form by G√∂del by 1931. But he neither publicized nor published the result; the biases logicians'),  \\nDocument(metadata={'Header 1': 'Kurt G√∂del', 'Header 2': '2. G√∂del‚Äôs Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='result; the biases logicians had expressed at the time concerning the notion of truth, biases which came vehemently to the fore when Tarski announced his results on the undefinability of truth in formal systems 1935, may have served as a deterrent to G√∂del‚Äôs publication of that theorem.'),  \\nDocument(metadata={'Header 1': 'Kurt G√∂del', 'Header 2': '2. G√∂del‚Äôs Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.2 The proof of the First Incompleteness Theorem'}, page_content='We now describe the proof of the two theorems, formulating G√∂del‚Äôs results in Peano arithmetic. G√∂del himself used a system related to that defined in Principia Mathematica, but containing Peano arithmetic. In our presentation of the First and Second Incompleteness Theorems we refer to Peano arithmetic as P, following G√∂del‚Äôs notation.')]\"),\n",
       " Document(metadata={'Header 2': 'Using HTMLHeaderTextSplitter', 'Header 3': 'Limitations'}, page_content='Limitations'),\n",
       " Document(metadata={'Header 2': 'Using HTMLHeaderTextSplitter', 'Header 3': 'Limitations'}, page_content='\\u200b  \\nThere can be quite a bit of structural variation from one HTML document to another, and while will attempt to attach all \"relevant\" headers to any given chunk, it can sometimes miss certain headers. For example, the algorithm assumes an informational hierarchy in which headers are always at nodes \"above\" associated text, i.e. prior siblings, ancestors, and combinations thereof. In the following news article (as of the writing of this document), the document is structured such that the text of the top-level headline, while tagged \"h1\", is in a subtree from the text elements that we\\'d expect it to be ‚Äîso we can observe that the \"h1\" element and its associated text do not show up in the chunk metadata (but, where applicable, we do see \"h2\" and its associated text):  \\nHTMLHeaderTextSplitter  \\ndistinct  \\n\"above\"  \\nurl  \\n=  \\n\"https://www.cnn.com/2023/09/25/weather/el-nino-winter-us-climate/index.html\"  \\nheaders_to_split_on  \\n=  \\n[  \\n(  \\n\"h1\"  \\n,  \\n\"Header 1\"  \\n)  \\n,  \\n(  \\n\"h2\"  \\n,  \\n\"Header 2\"  \\n)  \\n,  \\n]  \\nhtml_splitter  \\n=  \\nHTMLHeaderTextSplitter  \\n(  \\nheaders_to_split_on  \\n)  \\nhtml_header_splits  \\n=  \\nhtml_splitter  \\n.  \\nsplit_text_from_url  \\n(  \\nurl  \\n)  \\nprint  \\n(  \\nhtml_header_splits  \\n[  \\n1  \\n]  \\n.  \\npage_content  \\n[  \\n:  \\n500  \\n]  \\n)  \\nNo two El Ni√±o winters are the same, but many have temperature and precipitation trends in common.  \\nAverage conditions during an El Ni√±o winter across the continental US.  \\nOne of the major reasons is the position of the jet stream, which often shifts south during an El Ni√±o winter. This shift typically brings wetter and cooler weather to the South while the North becomes drier and warmer, according to NOAA.  \\nBecause the jet stream is essentially a river of air that storms flow through, they c'),\n",
       " Document(metadata={'Header 2': 'Using HTMLSectionSplitter'}, page_content='Using HTMLSectionSplitter'),\n",
       " Document(metadata={'Header 2': 'Using HTMLSectionSplitter'}, page_content='\\u200b  \\nSimilar in concept to the , the is a \"structure-aware\" that splits text at the element level and adds metadata for each header \"relevant\" to any given chunk. It lets you split HTML by sections.  \\nHTMLHeaderTextSplitter  \\nHTMLSectionSplitter  \\ntext splitter  \\nIt can return chunks element by element or combine elements with the same metadata, with the objectives of (a) keeping related text grouped (more or less) semantically and (b) preserving context-rich information encoded in document structures.  \\nUse to provide an absolute path to transform the HTML so that it can detect sections based on provided tags. The default is to use the file in the directory. This is for converting the html to a format/layout that is easier to detect sections. For example, based on their font size can be converted to header tags to be detected as a section.  \\nxslt_path  \\nconverting_to_header.xslt  \\ndata_connection/document_transformers  \\nspan'),\n",
       " Document(metadata={'Header 2': 'Using HTMLSectionSplitter', 'Header 3': 'How to split HTML strings:'}, page_content='How to split HTML strings:'),\n",
       " Document(metadata={'Header 2': 'Using HTMLSectionSplitter', 'Header 3': 'How to split HTML strings:'}, page_content='\\u200b  \\nfrom  \\nlangchain_text_splitters  \\nimport  \\nHTMLSectionSplitter  \\nheaders_to_split_on  \\n=  \\n[  \\n(  \\n\"h1\"  \\n,  \\n\"Header 1\"  \\n)  \\n,  \\n(  \\n\"h2\"  \\n,  \\n\"Header 2\"  \\n)  \\n,  \\n]  \\nhtml_splitter  \\n=  \\nHTMLSectionSplitter  \\n(  \\nheaders_to_split_on  \\n)  \\nhtml_header_splits  \\n=  \\nhtml_splitter  \\n.  \\nsplit_text  \\n(  \\nhtml_string  \\n)  \\nhtml_header_splits  \\n[Document(metadata={\\'Header 1\\': \\'Main Title\\'}, page_content=\\'Main Title \\\\n This is an introductory paragraph with some basic content.\\'),  \\nDocument(metadata={\\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\"Section 1: Introduction \\\\n This section introduces the topic. Below is a list: \\\\n \\\\n First item \\\\n Second item \\\\n Third item with  bold text  and  a link \\\\n \\\\n \\\\n Subsection 1.1: Details \\\\n This subsection provides additional details. Here\\'s a table: \\\\n \\\\n \\\\n \\\\n Header 1 \\\\n Header 2 \\\\n Header 3 \\\\n \\\\n \\\\n \\\\n \\\\n Row 1, Cell 1 \\\\n Row 1, Cell 2 \\\\n Row 1, Cell 3 \\\\n \\\\n \\\\n Row 2, Cell 1 \\\\n Row 2, Cell 2 \\\\n Row 2, Cell 3\"),  \\nDocument(metadata={\\'Header 2\\': \\'Section 2: Media Content\\'}, page_content=\\'Section 2: Media Content \\\\n This section contains an image and a video: \\\\n \\\\n \\\\n      Your browser does not support the video tag.\\'),  \\nDocument(metadata={\\'Header 2\\': \\'Section 3: Code Example\\'}, page_content=\\'Section 3: Code Example \\\\n This section contains a code block: \\\\n \\\\n    <div>\\\\n      <p>This is a paragraph inside a div.</p>\\\\n    </div>\\'),  \\nDocument(metadata={\\'Header 2\\': \\'Conclusion\\'}, page_content=\\'Conclusion \\\\n This is the conclusion of the document.\\')]'),\n",
       " Document(metadata={'Header 2': 'Using HTMLSectionSplitter', 'Header 3': 'How to constrain chunk sizes:'}, page_content='How to constrain chunk sizes:'),\n",
       " Document(metadata={'Header 2': 'Using HTMLSectionSplitter', 'Header 3': 'How to constrain chunk sizes:'}, page_content='\\u200b  \\ncan be used with other text splitters as part of a chunking pipeline. Internally, it uses the when the section size is larger than the chunk size. It also considers the font size of the text to determine whether it is a section or not based on the determined font size threshold.  \\nHTMLSectionSplitter  \\nRecursiveCharacterTextSplitter  \\nfrom  \\nlangchain_text_splitters  \\nimport  \\nRecursiveCharacterTextSplitter  \\nheaders_to_split_on  \\n=  \\n[  \\n(  \\n\"h1\"  \\n,  \\n\"Header 1\"  \\n)  \\n,  \\n(  \\n\"h2\"  \\n,  \\n\"Header 2\"  \\n)  \\n,  \\n(  \\n\"h3\"  \\n,  \\n\"Header 3\"  \\n)  \\n,  \\n]  \\nhtml_splitter  \\n=  \\nHTMLSectionSplitter  \\n(  \\nheaders_to_split_on  \\n)  \\nhtml_header_splits  \\n=  \\nhtml_splitter  \\n.  \\nsplit_text  \\n(  \\nhtml_string  \\n)  \\nchunk_size  \\n=  \\n50  \\nchunk_overlap  \\n=  \\n5  \\ntext_splitter  \\n=  \\nRecursiveCharacterTextSplitter  \\n(  \\nchunk_size  \\n=  \\nchunk_size  \\n,  \\nchunk_overlap  \\n=  \\nchunk_overlap  \\n)  \\n# Split  \\nsplits  \\n=  \\ntext_splitter  \\n.  \\nsplit_documents  \\n(  \\nhtml_header_splits  \\n)  \\nsplits  \\n[Document(metadata={\\'Header 1\\': \\'Main Title\\'}, page_content=\\'Main Title\\'),  \\nDocument(metadata={\\'Header 1\\': \\'Main Title\\'}, page_content=\\'This is an introductory paragraph with some\\'),  \\nDocument(metadata={\\'Header 1\\': \\'Main Title\\'}, page_content=\\'some basic content.\\'),  \\nDocument(metadata={\\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\\'Section 1: Introduction\\'),  \\nDocument(metadata={\\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\\'This section introduces the topic. Below is a\\'),  \\nDocument(metadata={\\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\\'is a list:\\'),  \\nDocument(metadata={\\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\\'First item \\\\n Second item\\'),  \\nDocument(metadata={\\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\\'Third item with  bold text  and  a link\\'),  \\nDocument(metadata={\\'Header 3\\': \\'Subsection 1.1: Details\\'}, page_content=\\'Subsection 1.1: Details\\'),  \\nDocument(metadata={\\'Header 3\\': \\'Subsection 1.1: Details\\'}, page_content=\\'This subsection provides additional details.\\'),  \\nDocument(metadata={\\'Header 3\\': \\'Subsection 1.1: Details\\'}, page_content=\"Here\\'s a table:\"),  \\nDocument(metadata={\\'Header 3\\': \\'Subsection 1.1: Details\\'}, page_content=\\'Header 1 \\\\n Header 2 \\\\n Header 3\\'),  \\nDocument(metadata={\\'Header 3\\': \\'Subsection 1.1: Details\\'}, page_content=\\'Row 1, Cell 1 \\\\n Row 1, Cell 2\\'),  \\nDocument(metadata={\\'Header 3\\': \\'Subsection 1.1: Details\\'}, page_content=\\'Row 1, Cell 3 \\\\n \\\\n \\\\n Row 2, Cell 1\\'),  \\nDocument(metadata={\\'Header 3\\': \\'Subsection 1.1: Details\\'}, page_content=\\'Row 2, Cell 2 \\\\n Row 2, Cell 3\\'),  \\nDocument(metadata={\\'Header 2\\': \\'Section 2: Media Content\\'}, page_content=\\'Section 2: Media Content\\'),  \\nDocument(metadata={\\'Header 2\\': \\'Section 2: Media Content\\'}, page_content=\\'This section contains an image and a video:\\'),  \\nDocument(metadata={\\'Header 2\\': \\'Section 2: Media Content\\'}, page_content=\\'Your browser does not support the video\\'),  \\nDocument(metadata={\\'Header 2\\': \\'Section 2: Media Content\\'}, page_content=\\'tag.\\'),  \\nDocument(metadata={\\'Header 2\\': \\'Section 3: Code Example\\'}, page_content=\\'Section 3: Code Example\\'),  \\nDocument(metadata={\\'Header 2\\': \\'Section 3: Code Example\\'}, page_content=\\'This section contains a code block: \\\\n \\\\n    <div>\\'),  \\nDocument(metadata={\\'Header 2\\': \\'Section 3: Code Example\\'}, page_content=\\'<p>This is a paragraph inside a div.</p>\\'),  \\nDocument(metadata={\\'Header 2\\': \\'Section 3: Code Example\\'}, page_content=\\'</div>\\'),  \\nDocument(metadata={\\'Header 2\\': \\'Conclusion\\'}, page_content=\\'Conclusion\\'),  \\nDocument(metadata={\\'Header 2\\': \\'Conclusion\\'}, page_content=\\'This is the conclusion of the document.\\')]'),\n",
       " Document(metadata={'Header 2': 'Using HTMLSemanticPreservingSplitter'}, page_content='Using HTMLSemanticPreservingSplitter'),\n",
       " Document(metadata={'Header 2': 'Using HTMLSemanticPreservingSplitter'}, page_content='\\u200b  \\nThe is designed to split HTML content into manageable chunks while preserving the semantic structure of important elements like tables, lists, and other HTML components. This ensures that such elements are not split across chunks, causing loss of contextual relevancy such as table headers, list headers etc.  \\nHTMLSemanticPreservingSplitter  \\nThis splitter is designed at its heart, to create contextually relevant chunks. General Recursive splitting with can cause tables, lists and other structured elements to be split in the middle, losing significant context and creating bad chunks.  \\nHTMLHeaderTextSplitter  \\nThe is essential for splitting HTML content that includes structured elements like tables and lists, especially when it\\'s critical to preserve these elements intact. Additionally, its ability to define custom handlers for specific HTML tags makes it a versatile tool for processing complex HTML documents.  \\nHTMLSemanticPreservingSplitter  \\n: is not a definite maximum size of a chunk, the calculation of max size, occurs when the preserved content is not apart of the chunk, to ensure it is not split. When we add the preserved data back in to the chunk, there is a chance the chunk size will exceed the . This is crucial to ensure we maintain the structure of the original document  \\nIMPORTANT  \\nmax_chunk_size  \\nmax_chunk_size  \\ninfo  \\nNotes:  \\nWe have defined a custom handler to re-format the contents of code blocks  \\nWe defined a deny list for specific html elements, to decompose them and their contents pre-processing  \\nWe have intentionally set a small chunk size to demonstrate the non-splitting of elements  \\n# BeautifulSoup is required to use the custom handlers  \\nfrom  \\nbs4  \\nimport  \\nTag  \\nfrom  \\nlangchain_text_splitters  \\nimport  \\nHTMLSemanticPreservingSplitter  \\nheaders_to_split_on  \\n=  \\n[  \\n(  \\n\"h1\"  \\n,  \\n\"Header 1\"  \\n)  \\n,  \\n(  \\n\"h2\"  \\n,  \\n\"Header 2\"  \\n)  \\n,  \\n]  \\ndef  \\ncode_handler  \\n(  \\nelement  \\n:  \\nTag  \\n)  \\n-  \\n>  \\nstr  \\n:  \\ndata_lang  \\n=  \\nelement  \\n.  \\nget  \\n(  \\n\"data-lang\"  \\n)  \\ncode_format  \\n=  \\nf\"<code:  \\n{  \\ndata_lang  \\n}  \\n>  \\n{  \\nelement  \\n.  \\nget_text  \\n(  \\n)  \\n}  \\n</code>\"  \\nreturn  \\ncode_format  \\nsplitter  \\n=  \\nHTMLSemanticPreservingSplitter  \\n(  \\nheaders_to_split_on  \\n=  \\nheaders_to_split_on  \\n,  \\nseparators  \\n=  \\n[  \\n\"\\\\n\\\\n\"  \\n,  \\n\"\\\\n\"  \\n,  \\n\". \"  \\n,  \\n\"! \"  \\n,  \\n\"? \"  \\n]  \\n,  \\nmax_chunk_size  \\n=  \\n50  \\n,  \\npreserve_images  \\n=  \\nTrue  \\n,  \\npreserve_videos  \\n=  \\nTrue  \\n,  \\nelements_to_preserve  \\n=  \\n[  \\n\"table\"  \\n,  \\n\"ul\"  \\n,  \\n\"ol\"  \\n,  \\n\"code\"  \\n]  \\n,  \\ndenylist_tags  \\n=  \\n[  \\n\"script\"  \\n,  \\n\"style\"  \\n,  \\n\"head\"  \\n]  \\n,  \\ncustom_handlers  \\n=  \\n{  \\n\"code\"  \\n:  \\ncode_handler  \\n}  \\n,  \\n)  \\ndocuments  \\n=  \\nsplitter  \\n.  \\nsplit_text  \\n(  \\nhtml_string  \\n)  \\ndocuments  \\n[Document(metadata={\\'Header 1\\': \\'Main Title\\'}, page_content=\\'This is an introductory paragraph with some basic content.\\'),  \\nDocument(metadata={\\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\\'This section introduces the topic\\'),  \\nDocument(metadata={\\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\\'. Below is a list: First item Second item Third item with bold text and a link Subsection 1.1: Details This subsection provides additional details\\'),  \\nDocument(metadata={\\'Header 2\\': \\'Section 1: Introduction\\'}, page_content=\". Here\\'s a table: Header 1 Header 2 Header 3 Row 1, Cell 1 Row 1, Cell 2 Row 1, Cell 3 Row 2, Cell 1 Row 2, Cell 2 Row 2, Cell 3\"),  \\nDocument(metadata={\\'Header 2\\': \\'Section 2: Media Content\\'}, page_content=\\'This section contains an image and a video: ![image:example_image_link.mp4](example_image_link.mp4) ![video:example_video_link.mp4](example_video_link.mp4)\\'),  \\nDocument(metadata={\\'Header 2\\': \\'Section 3: Code Example\\'}, page_content=\\'This section contains a code block: <code:html> <div> <p>This is a paragraph inside a div.</p> </div> </code>\\'),  \\nDocument(metadata={\\'Header 2\\': \\'Conclusion\\'}, page_content=\\'This is the conclusion of the document.\\')]'),\n",
       " Document(metadata={'Header 2': 'Using HTMLSemanticPreservingSplitter', 'Header 3': 'Preserving Tables and Lists'}, page_content='Preserving Tables and Lists'),\n",
       " Document(metadata={'Header 2': 'Using HTMLSemanticPreservingSplitter', 'Header 3': 'Preserving Tables and Lists'}, page_content='\\u200b  \\nIn this example, we will demonstrate how the can preserve a table and a large list within an HTML document. The chunk size will be set to 50 characters to illustrate how the splitter ensures that these elements are not split, even when they exceed the maximum defined chunk size.  \\nHTMLSemanticPreservingSplitter  \\nfrom  \\nlangchain_text_splitters  \\nimport  \\nHTMLSemanticPreservingSplitter  \\nhtml_string  \\n=  \\n\"\"\"  \\n<!DOCTYPE html>  \\n<html>  \\n<body>  \\n<div>  \\n<h1>Section 1</h1>  \\n<p>This section contains an important table and list that should not be split across chunks.</p>  \\n<table>  \\n<tr>  \\n<th>Item</th>  \\n<th>Quantity</th>  \\n<th>Price</th>  \\n</tr>  \\n<tr>  \\n<td>Apples</td>  \\n<td>10</td>  \\n<td>$1.00</td>  \\n</tr>  \\n<tr>  \\n<td>Oranges</td>  \\n<td>5</td>  \\n<td>$0.50</td>  \\n</tr>  \\n<tr>  \\n<td>Bananas</td>  \\n<td>50</td>  \\n<td>$1.50</td>  \\n</tr>  \\n</table>  \\n<h2>Subsection 1.1</h2>  \\n<p>Additional text in subsection 1.1 that is separated from the table and list.</p>  \\n<p>Here is a detailed list:</p>  \\n<ul>  \\n<li>Item 1: Description of item 1, which is quite detailed and important.</li>  \\n<li>Item 2: Description of item 2, which also contains significant information.</li>  \\n<li>Item 3: Description of item 3, another item that we don\\'t want to split across chunks.</li>  \\n</ul>  \\n</div>  \\n</body>  \\n</html>  \\n\"\"\"  \\nheaders_to_split_on  \\n=  \\n[  \\n(  \\n\"h1\"  \\n,  \\n\"Header 1\"  \\n)  \\n,  \\n(  \\n\"h2\"  \\n,  \\n\"Header 2\"  \\n)  \\n]  \\nsplitter  \\n=  \\nHTMLSemanticPreservingSplitter  \\n(  \\nheaders_to_split_on  \\n=  \\nheaders_to_split_on  \\n,  \\nmax_chunk_size  \\n=  \\n50  \\n,  \\nelements_to_preserve  \\n=  \\n[  \\n\"table\"  \\n,  \\n\"ul\"  \\n]  \\n,  \\n)  \\ndocuments  \\n=  \\nsplitter  \\n.  \\nsplit_text  \\n(  \\nhtml_string  \\n)  \\nprint  \\n(  \\ndocuments  \\n)  \\n[Document(metadata={\\'Header 1\\': \\'Section 1\\'}, page_content=\\'This section contains an important table and list\\'), Document(metadata={\\'Header 1\\': \\'Section 1\\'}, page_content=\\'that should not be split across chunks.\\'), Document(metadata={\\'Header 1\\': \\'Section 1\\'}, page_content=\\'Item Quantity Price Apples 10 $1.00 Oranges 5 $0.50 Bananas 50 $1.50\\'), Document(metadata={\\'Header 2\\': \\'Subsection 1.1\\'}, page_content=\\'Additional text in subsection 1.1 that is\\'), Document(metadata={\\'Header 2\\': \\'Subsection 1.1\\'}, page_content=\\'separated from the table and list. Here is a\\'), Document(metadata={\\'Header 2\\': \\'Subsection 1.1\\'}, page_content=\"detailed list: Item 1: Description of item 1, which is quite detailed and important. Item 2: Description of item 2, which also contains significant information. Item 3: Description of item 3, another item that we don\\'t want to split across chunks.\")]'),\n",
       " Document(metadata={'Header 2': 'Using HTMLSemanticPreservingSplitter', 'Header 3': 'Preserving Tables and Lists', 'Header 4': 'Explanation'}, page_content='Explanation'),\n",
       " Document(metadata={'Header 2': 'Using HTMLSemanticPreservingSplitter', 'Header 3': 'Preserving Tables and Lists', 'Header 4': 'Explanation'}, page_content='\\u200b  \\nIn this example, the ensures that the entire table and the unordered list ( ) are preserved within their respective chunks. Even though the chunk size is set to 50 characters, the splitter recognizes that these elements should not be split and keeps them intact.  \\nHTMLSemanticPreservingSplitter  \\n<ul>  \\nThis is particularly important when dealing with data tables or lists, where splitting the content could lead to loss of context or confusion. The resulting objects retain the full structure of these elements, ensuring that the contextual relevance of the information is maintained.  \\nDocument'),\n",
       " Document(metadata={'Header 2': 'Using HTMLSemanticPreservingSplitter', 'Header 3': 'Using a Custom Handler'}, page_content='Using a Custom Handler'),\n",
       " Document(metadata={'Header 2': 'Using HTMLSemanticPreservingSplitter', 'Header 3': 'Using a Custom Handler'}, page_content='\\u200b  \\nThe allows you to define custom handlers for specific HTML elements. Some platforms, have custom HTML tags that are not natively parsed by , when this occurs, you can utilize custom handlers to add the formatting logic easily.  \\nHTMLSemanticPreservingSplitter  \\nBeautifulSoup  \\nThis can be particularly useful for elements that require special processing, such as tags or specific \\'data-\\' elements. In this example, we\\'ll create a custom handler for tags that converts them into Markdown-like links.  \\n<iframe>  \\niframe  \\ndef  \\ncustom_iframe_extractor  \\n(  \\niframe_tag  \\n)  \\n:  \\niframe_src  \\n=  \\niframe_tag  \\n.  \\nget  \\n(  \\n\"src\"  \\n,  \\n\"\"  \\n)  \\nreturn  \\nf\"[iframe:  \\n{  \\niframe_src  \\n}  \\n](  \\n{  \\niframe_src  \\n}  \\n)\"  \\nsplitter  \\n=  \\nHTMLSemanticPreservingSplitter  \\n(  \\nheaders_to_split_on  \\n=  \\nheaders_to_split_on  \\n,  \\nmax_chunk_size  \\n=  \\n50  \\n,  \\nseparators  \\n=  \\n[  \\n\"\\\\n\\\\n\"  \\n,  \\n\"\\\\n\"  \\n,  \\n\". \"  \\n]  \\n,  \\nelements_to_preserve  \\n=  \\n[  \\n\"table\"  \\n,  \\n\"ul\"  \\n,  \\n\"ol\"  \\n]  \\n,  \\ncustom_handlers  \\n=  \\n{  \\n\"iframe\"  \\n:  \\ncustom_iframe_extractor  \\n}  \\n,  \\n)  \\nhtml_string  \\n=  \\n\"\"\"  \\n<!DOCTYPE html>  \\n<html>  \\n<body>  \\n<div>  \\n<h1>Section with Iframe</h1>  \\n<iframe src=\"https://example.com/embed\"></iframe>  \\n<p>Some text after the iframe.</p>  \\n<ul>  \\n<li>Item 1: Description of item 1, which is quite detailed and important.</li>  \\n<li>Item 2: Description of item 2, which also contains significant information.</li>  \\n<li>Item 3: Description of item 3, another item that we don\\'t want to split across chunks.</li>  \\n</ul>  \\n</div>  \\n</body>  \\n</html>  \\n\"\"\"  \\ndocuments  \\n=  \\nsplitter  \\n.  \\nsplit_text  \\n(  \\nhtml_string  \\n)  \\nprint  \\n(  \\ndocuments  \\n)  \\n[Document(metadata={\\'Header 1\\': \\'Section with Iframe\\'}, page_content=\\'[iframe:https://example.com/embed](https://example.com/embed) Some text after the iframe\\'), Document(metadata={\\'Header 1\\': \\'Section with Iframe\\'}, page_content=\". Item 1: Description of item 1, which is quite detailed and important. Item 2: Description of item 2, which also contains significant information. Item 3: Description of item 3, another item that we don\\'t want to split across chunks.\")]'),\n",
       " Document(metadata={'Header 2': 'Using HTMLSemanticPreservingSplitter', 'Header 3': 'Using a Custom Handler', 'Header 4': 'Explanation'}, page_content='Explanation'),\n",
       " Document(metadata={'Header 2': 'Using HTMLSemanticPreservingSplitter', 'Header 3': 'Using a Custom Handler', 'Header 4': 'Explanation'}, page_content='\\u200b  \\nIn this example, we defined a custom handler for tags that converts them into Markdown-like links. When the splitter processes the HTML content, it uses this custom handler to transform the tags while preserving other elements like tables and lists. The resulting objects show how the iframe is handled according to the custom logic you provided.  \\niframe  \\niframe  \\nDocument  \\n: When presvering items such as links, you should be mindful not to include in your separators, or leave separators blank. splits on full stop, which will cut links in half. Ensure you provide a separator list with instead.  \\nImportant  \\n.  \\nRecursiveCharacterTextSplitter  \\n.'),\n",
       " Document(metadata={'Header 2': 'Using HTMLSemanticPreservingSplitter', 'Header 3': 'Using a custom handler to analyze an image with an LLM'}, page_content='Using a custom handler to analyze an image with an LLM'),\n",
       " Document(metadata={'Header 2': 'Using HTMLSemanticPreservingSplitter', 'Header 3': 'Using a custom handler to analyze an image with an LLM'}, page_content='\\u200b  \\nWith custom handler\\'s, we can also override the default processing for any element. A great example of this, is inserting semantic analysis of an image within a document, directly in the chunking flow.  \\nSince our function is called when the tag is discovered, we can override the tag and turn off to insert any content we would like to embed in our chunks.  \\n<img>  \\npreserve_images  \\n\"\"\"This example assumes you have helper methods `load_image_from_url` and an LLM agent `llm` that can process image data.\"\"\"  \\nfrom  \\nlangchain  \\n.  \\nagents  \\nimport  \\nAgentExecutor  \\n# This example needs to be replaced with your own agent  \\nllm  \\n=  \\nAgentExecutor  \\n(  \\n.  \\n.  \\n.  \\n)  \\n# This method is a placeholder for loading image data from a URL and is not implemented here  \\ndef  \\nload_image_from_url  \\n(  \\nimage_url  \\n:  \\nstr  \\n)  \\n-  \\n>  \\nbytes  \\n:  \\n# Assuming this method fetches the image data from the URL  \\nreturn  \\nb\"image_data\"  \\nhtml_string  \\n=  \\n\"\"\"  \\n<!DOCTYPE html>  \\n<html>  \\n<body>  \\n<div>  \\n<h1>Section with Image and Link</h1>  \\n<p>  \\n<img src=\"https://example.com/image.jpg\" alt=\"An example image\" />  \\nSome text after the image.  \\n</p>  \\n<ul>  \\n<li>Item 1: Description of item 1, which is quite detailed and important.</li>  \\n<li>Item 2: Description of item 2, which also contains significant information.</li>  \\n<li>Item 3: Description of item 3, another item that we don\\'t want to split across chunks.</li>  \\n</ul>  \\n</div>  \\n</body>  \\n</html>  \\n\"\"\"  \\ndef  \\ncustom_image_handler  \\n(  \\nimg_tag  \\n)  \\n-  \\n>  \\nstr  \\n:  \\nimg_src  \\n=  \\nimg_tag  \\n.  \\nget  \\n(  \\n\"src\"  \\n,  \\n\"\"  \\n)  \\nimg_alt  \\n=  \\nimg_tag  \\n.  \\nget  \\n(  \\n\"alt\"  \\n,  \\n\"No alt text provided\"  \\n)  \\nimage_data  \\n=  \\nload_image_from_url  \\n(  \\nimg_src  \\n)  \\nsemantic_meaning  \\n=  \\nllm  \\n.  \\ninvoke  \\n(  \\nimage_data  \\n)  \\nmarkdown_text  \\n=  \\nf\"[Image Alt Text:  \\n{  \\nimg_alt  \\n}  \\n| Image Source:  \\n{  \\nimg_src  \\n}  \\n| Image Semantic Meaning:  \\n{  \\nsemantic_meaning  \\n}  \\n]\"  \\nreturn  \\nmarkdown_text  \\nsplitter  \\n=  \\nHTMLSemanticPreservingSplitter  \\n(  \\nheaders_to_split_on  \\n=  \\nheaders_to_split_on  \\n,  \\nmax_chunk_size  \\n=  \\n50  \\n,  \\nseparators  \\n=  \\n[  \\n\"\\\\n\\\\n\"  \\n,  \\n\"\\\\n\"  \\n,  \\n\". \"  \\n]  \\n,  \\nelements_to_preserve  \\n=  \\n[  \\n\"ul\"  \\n]  \\n,  \\npreserve_images  \\n=  \\nFalse  \\n,  \\ncustom_handlers  \\n=  \\n{  \\n\"img\"  \\n:  \\ncustom_image_handler  \\n}  \\n,  \\n)  \\ndocuments  \\n=  \\nsplitter  \\n.  \\nsplit_text  \\n(  \\nhtml_string  \\n)  \\nprint  \\n(  \\ndocuments  \\n)  \\n[Document(metadata={\\'Header 1\\': \\'Section with Image and Link\\'}, page_content=\\'[Image Alt Text: An example image | Image Source: https://example.com/image.jpg | Image Semantic Meaning: semantic-meaning] Some text after the image\\'),  \\nDocument(metadata={\\'Header 1\\': \\'Section with Image and Link\\'}, page_content=\". Item 1: Description of item 1, which is quite detailed and important. Item 2: Description of item 2, which also contains significant information. Item 3: Description of item 3, another item that we don\\'t want to split across chunks.\")]'),\n",
       " Document(metadata={'Header 2': 'Using HTMLSemanticPreservingSplitter', 'Header 3': 'Using a custom handler to analyze an image with an LLM', 'Header 4': 'Explanation:'}, page_content='Explanation:'),\n",
       " Document(metadata={}, page_content='\\u200b  \\nWith our custom handler written to extract the specific fields from a element in HTML, we can further process the data with our agent, and insert the result directly into our chunk. It is important to ensure is set to otherwise the default processing of fields will take place.  \\n<img>  \\npreserve_images  \\nFalse  \\n<img>  \\nEdit this page  \\nPrevious  \\nHow to split text by tokens  \\nNext  \\nHow to do question answering over CSVs  \\nOverview of the Splitters  \\nHTMLHeaderTextSplitter  \\nHTMLSectionSplitter  \\nHTMLSemanticPreservingSplitter  \\nChoosing the Right Splitter  \\nExample HTML Document  \\nUsing HTMLHeaderTextSplitter  \\nHow to split from a URL or HTML file:  \\nHow to constrain chunk sizes:  \\nLimitations  \\nUsing HTMLSectionSplitter  \\nHow to split HTML strings:  \\nHow to constrain chunk sizes:  \\nUsing HTMLSemanticPreservingSplitter  \\nPreserving Tables and Lists  \\nUsing a Custom Handler  \\nUsing a custom handler to analyze an image with an LLM  \\nCommunity  \\nLangChain Forum  \\nTwitter  \\nSlack  \\nGitHub  \\nOrganization  \\nPython  \\nJS/TS  \\nMore  \\nHomepage  \\nBlog  \\nYouTube  \\nCopyright ¬© 2025 LangChain, Inc.')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_text_splitters import HTMLHeaderTextSplitter,HTMLSectionSplitter,HTMLSemanticPreservingSplitter\n",
    "\n",
    "\"\"\" Splits HTML text based on header tags (e.g.,<h1>, <h2>, <h3>, etc.),\n",
    " and adds metadata for each header relevant to any given chunk\"\"\"\n",
    "\n",
    "url='https://python.langchain.com/docs/how_to/split_html/'\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "    (\"h4\", \"Header 4\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "\n",
    "# for local file use html_splitter.split_text_from_file(<path_to_file>)\n",
    "html_header_splits = html_splitter.split_text_from_url(url)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d59cc5",
   "metadata": {},
   "source": [
    "# 6. RecursiveJsonSplitter\n",
    "### It is used to split nested JSON data into smaller, manageable chunks while preserving hierarchical structure and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9034a0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "{'university': 'Sheffield Hallam University', 'location': 'Sheffield, United Kingdom'}\n",
      "\n",
      "Chunk 2:\n",
      "{'departments': {'0': {'name': 'Computing and AI'}}}\n",
      "\n",
      "Chunk 3:\n",
      "{'departments': {'0': {'courses': {'0': {'course_name': 'MSc Big Data Analytics'}}}}}\n",
      "\n",
      "Chunk 4:\n",
      "{'departments': {'0': {'courses': {'0': {'modules': {'0': {'name': 'Machine Learning', 'credits': 20}, '1': {'name': 'Deep Learning', 'credits': 20}}}}}}}\n",
      "\n",
      "Chunk 5:\n",
      "{'departments': {'0': {'courses': {'0': {'modules': {'2': {'name': 'Big Data Engineering', 'credits': 20}}}}}}}\n",
      "\n",
      "Chunk 6:\n",
      "{'departments': {'0': {'courses': {'1': {'course_name': 'MSc Cyber Security', 'modules': {'0': {'name': 'Network Security', 'credits': 20}, '1': {'name': 'Cryptography', 'credits': 20}}}}}}}\n",
      "\n",
      "Chunk 7:\n",
      "{'departments': {'1': {'name': 'Business and Management'}}}\n",
      "\n",
      "Chunk 8:\n",
      "{'departments': {'1': {'courses': {'0': {'course_name': 'MBA International Business', 'modules': {'0': {'name': 'Global Strategy', 'credits': 20}, '1': {'name': 'Leadership', 'credits': 20}}}}}}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveJsonSplitter\n",
    "\n",
    "# Sample nested JSON data\n",
    "university_data = {\n",
    "    \"university\": \"Sheffield Hallam University\",\n",
    "    \"location\": \"Sheffield, United Kingdom\",\n",
    "    \"departments\": [\n",
    "        {\n",
    "            \"name\": \"Computing and AI\",\n",
    "            \"courses\": [\n",
    "                {\n",
    "                    \"course_name\": \"MSc Big Data Analytics\",\n",
    "                    \"modules\": [\n",
    "                        {\"name\": \"Machine Learning\", \"credits\": 20},\n",
    "                        {\"name\": \"Deep Learning\", \"credits\": 20},\n",
    "                        {\"name\": \"Big Data Engineering\", \"credits\": 20}\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"course_name\": \"MSc Cyber Security\",\n",
    "                    \"modules\": [\n",
    "                        {\"name\": \"Network Security\", \"credits\": 20},\n",
    "                        {\"name\": \"Cryptography\", \"credits\": 20}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Business and Management\",\n",
    "            \"courses\": [\n",
    "                {\n",
    "                    \"course_name\": \"MBA International Business\",\n",
    "                    \"modules\": [\n",
    "                        {\"name\": \"Global Strategy\", \"credits\": 20},\n",
    "                        {\"name\": \"Leadership\", \"credits\": 20}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create RecursiveJsonSplitter instance\n",
    "splitter = RecursiveJsonSplitter(max_chunk_size=200)\n",
    "\n",
    "# Split JSON into smaller structured chunks\n",
    "chunks = splitter.split_json(university_data,convert_lists=True)\n",
    "\n",
    "# Print results\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {idx+1}:\\n{chunk}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d98514",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
