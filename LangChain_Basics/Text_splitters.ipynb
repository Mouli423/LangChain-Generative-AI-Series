{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c275df15",
   "metadata": {},
   "source": [
    "# LangChain Text Splitters – A Hands-On Guide\n",
    "\n",
    "This repository demonstrates how **LangChain Text Splitters** are used to divide large documents into smaller, meaningful chunks for better embeddings and retrieval in **Generative AI pipelines**.\n",
    "\n",
    "## Why Text Splitting?\n",
    "\n",
    "LLMs (like GPT-5, Groq, Gemini or Claude) have input size limits (token windows). Large documents can exceed those limits, causing:\n",
    "- Lost context or truncated responses  \n",
    "- Poor embedding quality  \n",
    "- Slow and inefficient retrieval  \n",
    "\n",
    "Text splitters solve this by **dividing text into overlapping, context-preserving chunks** before embedding or retrieval.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2844ab54",
   "metadata": {},
   "source": [
    "# 1. TokenTextSplitter \n",
    "### It splits the data by token count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b3584a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './speech.txt'}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content=' those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\n…\\n\\nIt will be all the easier for')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "loader = TextLoader(\"./speech.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=25)\n",
    "\n",
    "texts = text_splitter.split_documents(docs)\n",
    "texts[:2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5543a057",
   "metadata": {},
   "source": [
    "# 2. CharacterTextSplitter - The text is split based on the number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf8ac5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 470, which is longer than the specified 75\n",
      "Created a chunk of size 347, which is longer than the specified 75\n",
      "Created a chunk of size 668, which is longer than the specified 75\n",
      "Created a chunk of size 982, which is longer than the specified 75\n",
      "Created a chunk of size 789, which is longer than the specified 75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './speech.txt'}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='Just because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='…')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter=CharacterTextSplitter(\n",
    "    separator='\\n',\n",
    "    chunk_size=75,\n",
    "    chunk_overlap=15\n",
    ")\n",
    "\n",
    "texts=text_splitter.split_documents(docs)\n",
    "\n",
    "\"\"\"Text will be split only at new lines since we are using the new line (“\\n”) as the separator.\n",
    " If any chunk has a size more than 75 but no new lines in it, it will be returned as such.\"\"\"\n",
    "\n",
    "texts[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58194dac",
   "metadata": {},
   "source": [
    "# 3. RecursiveCharacterTextSplitter\n",
    "\n",
    "### This method uses multiple separators recursively to split the data until the chunk reaches the less than the chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf7c0bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './speech.txt'}, page_content='The world must be made safe for democracy.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='Its peace must be planted upon the tested foundations of political liberty.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='We have no selfish ends to serve.(?<=[.,])\\\\s+We desire no conquest,(?<=[.,])\\\\s+no dominion.'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='no dominion.(?<=[.,])\\\\s+We seek no indemnities for ourselves,'),\n",
       " Document(metadata={'source': './speech.txt'}, page_content='no material compensation for the sacrifices we shall freely make.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    separators=['\\n\\n',r\"(?<=[.,])\\s+\"],\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    is_separator_regex=True,\n",
    "    keep_separator=False\n",
    ")\n",
    "\n",
    "texts=text_splitter.split_documents(docs)\n",
    "texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4be306",
   "metadata": {},
   "source": [
    "# 4. MarkdownHeaderTextSplitter \n",
    "### It is used when the text follows a structured Markdown format —  \n",
    "### like articles, documentation, or hierarchical notes (with `#`, `##`, `###` headers, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dac7893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'LangChain TextSplitters'}, page_content='Text Splitters are most important elements the langchain pipeline which divides the data in meaningful chunks.'),\n",
       " Document(metadata={'Header 1': 'LangChain TextSplitters', 'Header 2': 'Types of TextSplitters'}, page_content='1.  Length-Based\\n2. Text Structured Based\\n3. Document Structured Based\\n4. Semantic Meaning Based'),\n",
       " Document(metadata={'Header 1': 'LangChain TextSplitters', 'Header 2': 'Types of TextSplitters', 'Header 3': 'Reasons to Splt documents'}, page_content='1. Handling non-uniform document lengths\\n2. Overcoming model limitations\\n3. Improving representaion quality\\n4. Optimizing Computational Resources')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "markdown_text = \"\"\"\n",
    "# LangChain TextSplitters\n",
    "Text Splitters are most important elements the langchain pipeline which divides the data in meaningful chunks.\n",
    "## Types of TextSplitters\n",
    "1.  Length-Based\n",
    "2. Text Structured Based\n",
    "3. Document Structured Based\n",
    "4. Semantic Meaning Based\n",
    "### Reasons to Splt documents\n",
    "1. Handling non-uniform document lengths\n",
    "2. Overcoming model limitations\n",
    "3. Improving representaion quality\n",
    "4. Optimizing Computational Resources\n",
    "\"\"\"\n",
    "\n",
    "splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=[(\"#\", \"Header 1\"), (\"##\", \"Header 2\"),('###','Header 3')]\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(markdown_text)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8697c65f",
   "metadata": {},
   "source": [
    "# 5. HTMLHeaderTextSplitter\n",
    "### It is a \"structure-aware\" text splitter that splits text at the HTML element level and adds metadata for each header \"relevant\" to any given chunk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a2a103",
   "metadata": {},
   "source": [
    "Choosing the Right Splitter\n",
    "- HTMLHeaderTextSplitter:When You need to split an HTML document based on its header hierarchy and maintain metadata about the headers.\n",
    "- Use HTMLSectionSplitter when: You need to split the document into larger, more general sections, possibly based on custom tags or font sizes.\n",
    "- Use HTMLSemanticPreservingSplitter when: You need to split the document into chunks while preserving semantic elements like tables and lists, ensuring that they are not split and that their context is maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee57736f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 2': 'Overview of the Splitters'}, page_content='Overview of the Splitters'),\n",
       " Document(metadata={'Header 2': 'Overview of the Splitters'}, page_content='\\u200b  \\nHTMLHeaderTextSplitter  \\n\\u200b  \\ninfo  \\nUseful when you want to preserve the hierarchical structure of a document based on its headings.  \\n: Splits HTML text based on header tags (e.g., , , , etc.), and adds metadata for each header relevant to any given chunk.  \\nDescription  \\n<h1>  \\n<h2>  \\n<h3>  \\n:  \\nCapabilities  \\nSplits text at the HTML element level.  \\nPreserves context-rich information encoded in document structures.  \\nCan return chunks element by element or combine elements with the same metadata.  \\nHTMLSectionSplitter  \\n\\u200b  \\ninfo  \\nUseful when you want to split HTML documents into larger sections, such as , , or custom-defined sections.  \\n<section>  \\n<div>  \\n: Similar to HTMLHeaderTextSplitter but focuses on splitting HTML into sections based on specified tags.  \\nDescription  \\n:  \\nCapabilities  \\nUses XSLT transformations to detect and split sections.  \\nInternally uses for large sections.  \\nRecursiveCharacterTextSplitter  \\nConsiders font sizes to determine sections.  \\nHTMLSemanticPreservingSplitter  \\n\\u200b  \\ninfo  \\nIdeal when you need to ensure that structured elements are not split across chunks, preserving contextual relevancy.  \\n: Splits HTML content into manageable chunks while preserving the semantic structure of important elements like tables, lists, and other HTML components.  \\nDescription  \\n:  \\nCapabilities  \\nPreserves tables, lists, and other specified HTML elements.  \\nAllows custom handlers for specific HTML tags.  \\nEnsures that the semantic meaning of the document is maintained.  \\nBuilt in normalization & stopword removal')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_text_splitters import HTMLHeaderTextSplitter,HTMLSectionSplitter,HTMLSemanticPreservingSplitter\n",
    "\n",
    "\"\"\" Splits HTML text based on header tags (e.g.,<h1>, <h2>, <h3>, etc.),\n",
    " and adds metadata for each header relevant to any given chunk\"\"\"\n",
    "\n",
    "url='https://python.langchain.com/docs/how_to/split_html/'\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "    (\"h4\", \"Header 4\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "\n",
    "# for local file use html_splitter.split_text_from_file(<path_to_file>)\n",
    "html_header_splits = html_splitter.split_text_from_url(url)\n",
    "html_header_splits[3:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d59cc5",
   "metadata": {},
   "source": [
    "# 6. RecursiveJsonSplitter\n",
    "### It is used to split nested JSON data into smaller, manageable chunks while preserving hierarchical structure and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9034a0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "{'university': 'Sheffield Hallam University', 'location': 'Sheffield, United Kingdom'}\n",
      "\n",
      "Chunk 2:\n",
      "{'departments': {'0': {'name': 'Computing and AI'}}}\n",
      "\n",
      "Chunk 3:\n",
      "{'departments': {'0': {'courses': {'0': {'course_name': 'MSc Big Data Analytics'}}}}}\n",
      "\n",
      "Chunk 4:\n",
      "{'departments': {'0': {'courses': {'0': {'modules': {'0': {'name': 'Machine Learning', 'credits': 20}, '1': {'name': 'Deep Learning', 'credits': 20}}}}}}}\n",
      "\n",
      "Chunk 5:\n",
      "{'departments': {'0': {'courses': {'0': {'modules': {'2': {'name': 'Big Data Engineering', 'credits': 20}}}}}}}\n",
      "\n",
      "Chunk 6:\n",
      "{'departments': {'0': {'courses': {'1': {'course_name': 'MSc Cyber Security', 'modules': {'0': {'name': 'Network Security', 'credits': 20}, '1': {'name': 'Cryptography', 'credits': 20}}}}}}}\n",
      "\n",
      "Chunk 7:\n",
      "{'departments': {'1': {'name': 'Business and Management'}}}\n",
      "\n",
      "Chunk 8:\n",
      "{'departments': {'1': {'courses': {'0': {'course_name': 'MBA International Business', 'modules': {'0': {'name': 'Global Strategy', 'credits': 20}, '1': {'name': 'Leadership', 'credits': 20}}}}}}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveJsonSplitter\n",
    "\n",
    "# Sample nested JSON data\n",
    "university_data = {\n",
    "    \"university\": \"Sheffield Hallam University\",\n",
    "    \"location\": \"Sheffield, United Kingdom\",\n",
    "    \"departments\": [\n",
    "        {\n",
    "            \"name\": \"Computing and AI\",\n",
    "            \"courses\": [\n",
    "                {\n",
    "                    \"course_name\": \"MSc Big Data Analytics\",\n",
    "                    \"modules\": [\n",
    "                        {\"name\": \"Machine Learning\", \"credits\": 20},\n",
    "                        {\"name\": \"Deep Learning\", \"credits\": 20},\n",
    "                        {\"name\": \"Big Data Engineering\", \"credits\": 20}\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"course_name\": \"MSc Cyber Security\",\n",
    "                    \"modules\": [\n",
    "                        {\"name\": \"Network Security\", \"credits\": 20},\n",
    "                        {\"name\": \"Cryptography\", \"credits\": 20}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Business and Management\",\n",
    "            \"courses\": [\n",
    "                {\n",
    "                    \"course_name\": \"MBA International Business\",\n",
    "                    \"modules\": [\n",
    "                        {\"name\": \"Global Strategy\", \"credits\": 20},\n",
    "                        {\"name\": \"Leadership\", \"credits\": 20}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create RecursiveJsonSplitter instance\n",
    "splitter = RecursiveJsonSplitter(max_chunk_size=200)\n",
    "\n",
    "# Split JSON into smaller structured chunks\n",
    "chunks = splitter.split_json(university_data,convert_lists=True)\n",
    "\n",
    "# Print results\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {idx+1}:\\n{chunk}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f478bf",
   "metadata": {},
   "source": [
    "# 7. SemanticChunker\n",
    "### it goes beyond syntax — they use embeddings and similarity to split text where context naturally shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d98514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n",
      "The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve.\n",
      "665\n",
      "We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them. Just because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\n",
      "1311\n",
      "…\n",
      "\n",
      "It will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us—however hard it may be for them, for the time being, to believe that this is spoken from our hearts. We have borne with their present government through all these bitter months because of that friendship—exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance.\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Create your HF_TOKEN from huggingface and store it in .env file\n",
    "\n",
    "\n",
    "semantic_splitter = SemanticChunker(HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2'), buffer_size=1,\n",
    "                                    breakpoint_threshold_type='percentile', breakpoint_threshold_amount=70)\n",
    "                                    \n",
    "texts = semantic_splitter.split_documents(docs)\n",
    "\n",
    "for text in texts[:3]:\n",
    "    print(len(text.page_content))\n",
    "    print(text.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583f893d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
