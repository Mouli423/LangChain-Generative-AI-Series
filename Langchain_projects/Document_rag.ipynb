{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8597f594",
   "metadata": {},
   "source": [
    "## Basic RAG (Retrieval-Augmented Generation) with LangChain\n",
    "\n",
    "- This project demonstrates the complete workflow of building a Basic RAG system using LangChain. It covers every step required to convert raw documents into a retrieval-enabled LLM application.\n",
    "\n",
    "- The purpose is to show the core workflow that connects:\n",
    "\n",
    "1. Document processing\n",
    "\n",
    "2. Embedding generation\n",
    "\n",
    "3. Vector database indexing\n",
    "\n",
    "4. Retrieval\n",
    "\n",
    "5. LLM-based answer generation\n",
    "\n",
    "- The goal is to create a simple but fully functional pipeline where a user can ask a question and the system retrieves relevant information from loaded documents to generate an accurate, grounded answer.\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "- RAG (Retrieval-Augmented Generation) enhances LLM responses by grounding them in external knowledge. Instead of relying solely on the model’s internal training data, the system retrieves relevant information from your documents and uses it during generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fe3290d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 16\n"
     ]
    }
   ],
   "source": [
    "# Loading environment variable of groq and huggingface model\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"HF_TOKEN\"]=os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6551bc41",
   "metadata": {},
   "source": [
    "### 1. Document Loading\n",
    "\n",
    "- Documents are ingested using LangChain's document loader utilities. This notebook supports text and PDF files but can be extended to other formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "77369670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../research_papers/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader=PyPDFLoader(\"../research_papers/attention.pdf\") # Replace with your file path\n",
    "docs=loader.load()\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4edb7a",
   "metadata": {},
   "source": [
    "### 2. Splitting into Chunks\n",
    "\n",
    "- Long documents are divided into manageable chunks using a text splitter with overlap to preserve context continuity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b377704f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../research_papers/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../research_papers/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='performing models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../research_papers/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='best models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=250)\n",
    "documents=text_splitter.split_documents(documents=docs)\n",
    "documents[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce603e3",
   "metadata": {},
   "source": [
    "### 3. Embeddings Generation\n",
    "\n",
    "- Each chunk is embedded into a high-dimensional vector representation. Embeddings translate natural language into numerical form for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "87be2636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689f5f94",
   "metadata": {},
   "source": [
    "### 4. Building a Vector Store\n",
    "\n",
    "- Chroma is used as the vector database to index and store all embeddings. It enables efficient similarity search across large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "03e665dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'creationdate': '2024-04-10T21:11:43+00:00', 'title': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 1, 'producer': 'pdfTeX-1.40.25', 'source': '../research_papers/attention.pdf', 'creator': 'LaTeX with hyperref', 'page_label': '2', 'trapped': '/False', 'author': '', 'total_pages': 15, 'subject': '', 'keywords': ''}, page_content='self-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       "  1.1027004718780518),\n",
       " (Document(metadata={'author': '', 'producer': 'pdfTeX-1.40.25', 'total_pages': 15, 'page_label': '2', 'keywords': '', 'trapped': '/False', 'moddate': '2024-04-10T21:11:43+00:00', 'title': '', 'creator': 'LaTeX with hyperref', 'source': '../research_papers/attention.pdf', 'creationdate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'page': 1}, page_content='self-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       "  1.1027004718780518),\n",
       " (Document(metadata={'title': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 1, 'total_pages': 15, 'trapped': '/False', 'creator': 'LaTeX with hyperref', 'source': '../research_papers/attention.pdf', 'keywords': '', 'page_label': '2', 'author': '', 'producer': 'pdfTeX-1.40.25', 'creationdate': '2024-04-10T21:11:43+00:00'}, page_content='self-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       "  1.1027004718780518),\n",
       " (Document(metadata={'keywords': '', 'title': '', 'page_label': '2', 'subject': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'producer': 'pdfTeX-1.40.25', 'page': 1, 'total_pages': 15, 'creationdate': '2024-04-10T21:11:43+00:00', 'moddate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'trapped': '/False', 'source': '../research_papers/attention.pdf', 'author': ''}, page_content='self-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       "  1.1027004718780518)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vector_store=Chroma.from_documents(documents=documents,embedding=embeddings)\n",
    "\n",
    "vector_store.similarity_search_with_score(\" self-Attention mechanisam\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2b8e99",
   "metadata": {},
   "source": [
    "### 5. LLM Model Selection\n",
    "\n",
    "- Open source Groq model is used as the llm model. Along with provide api key of groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2f5ca4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I help you today?', additional_kwargs={'reasoning_content': 'The user says \"hi\". Likely respond politely.'}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 72, 'total_tokens': 102, 'completion_time': 0.029977205, 'prompt_time': 0.003795284, 'queue_time': 0.13558457, 'total_time': 0.033772489, 'completion_tokens_details': {'reasoning_tokens': 12}}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_d6de37e6be', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--e53addc0-eb5c-4942-a9ad-f5c97c352b45-0', usage_metadata={'input_tokens': 72, 'output_tokens': 30, 'total_tokens': 102})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm=ChatGroq(model=\"openai/gpt-oss-20b\",api_key=groq_api_key)\n",
    "\n",
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a97ffb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\" \\nYou are an helpful assistant document Q and A tasks. please provide the response to the user questions.\\nIn which context is from the retriever. If you don't know the answer simply say i don't know.\\n\\n\\nHere is the context: \\n\\n{context}\\n\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Designing  the prompt template for the llm\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=\"\"\" \n",
    "You are an helpful assistant document Q and A tasks. please provide the response to the user questions.\n",
    "In which context is from the retriever. If you don't know the answer simply say i don't know.\n",
    "\n",
    "\n",
    "Here is the context: \n",
    "\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "\n",
    "    (\"system\",prompt),\n",
    "    (\"human\",\"{input}\")\n",
    "]\n",
    ")\n",
    "\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d39cde",
   "metadata": {},
   "source": [
    "### 6. Creating a Retriever\n",
    "\n",
    "- The retriever component handles similarity search. Given a user query, it returns the most semantically relevant chunks from the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f23cd6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievier=vector_store.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4cf511b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_classic.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "qanda_chain=create_stuff_documents_chain(llm,prompt_template) # It creates a chain for passing a list of documents to a model\n",
    "\n",
    "retriever_chain=create_retrieval_chain(retrievier,qanda_chain) # It creates a retrieval chain that retrieves documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd2be7a",
   "metadata": {},
   "source": [
    "### 7. Retrieval-Augmented Generation\n",
    "\n",
    "- The retrieved context is combined with the user query and passed to a chat model.\n",
    "This ensures the model’s answer is guided by the document content rather than relying solely on its internal knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "117a7e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'what is self-attention mechanism',\n",
       " 'context': [Document(metadata={'page_label': '5', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'trapped': '/False', 'source': '../research_papers/attention.pdf', 'producer': 'pdfTeX-1.40.25', 'moddate': '2024-04-10T21:11:43+00:00', 'keywords': '', 'creator': 'LaTeX with hyperref', 'page': 4, 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': '', 'total_pages': 15, 'subject': ''}, page_content='3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward'),\n",
       "  Document(metadata={'title': '', 'page_label': '5', 'author': '', 'moddate': '2024-04-10T21:11:43+00:00', 'keywords': '', 'page': 4, 'creator': 'LaTeX with hyperref', 'subject': '', 'source': '../research_papers/attention.pdf', 'trapped': '/False', 'creationdate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'total_pages': 15, 'producer': 'pdfTeX-1.40.25'}, page_content='3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward'),\n",
       "  Document(metadata={'page': 4, 'total_pages': 15, 'author': '', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'moddate': '2024-04-10T21:11:43+00:00', 'keywords': '', 'producer': 'pdfTeX-1.40.25', 'page_label': '5', 'title': '', 'subject': '', 'trapped': '/False', 'source': '../research_papers/attention.pdf', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5'}, page_content='3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward'),\n",
       "  Document(metadata={'trapped': '/False', 'author': '', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'keywords': '', 'source': '../research_papers/attention.pdf', 'page_label': '5', 'subject': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': '', 'total_pages': 15, 'page': 4, 'moddate': '2024-04-10T21:11:43+00:00', 'producer': 'pdfTeX-1.40.25'}, page_content='3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward')],\n",
       " 'answer': '**Self‑attention** (also called *intra‑attention*) is a core component of the Transformer architecture.  \\nIn a self‑attention layer every token (or position) in a sequence can “look at” every other token in the same sequence to gather contextual information.\\n\\nKey points:\\n\\n| Component | How it’s computed | What it represents |\\n|-----------|-------------------|--------------------|\\n| **Query (Q)** | Derived from the token’s own representation (e.g., by a linear projection) | The “request” or focus of the token |\\n| **Key (K)** | Derived from all tokens in the sequence (same linear projection as Q) | The “identifier” used to compare with queries |\\n| **Value (V)** | Derived from all tokens (same linear projection as K) | The actual information to be aggregated |\\n\\nThe attention score for a pair of tokens is the dot‑product of their Q and K vectors, usually scaled by the square root of the dimension and passed through a softmax to obtain a probability distribution. Each token then takes a weighted sum of all V vectors using these probabilities, producing a new representation that blends its own information with relevant context from the whole sequence.\\n\\nIn the Transformer, self‑attention is used both in the encoder (allowing each encoder position to attend to all other encoder positions) and in the decoder (where it is usually masked so that a position can only attend to positions **up to and including** itself, preventing future information leakage).\\n\\nThis mechanism enables the model to capture long‑range dependencies efficiently and is a fundamental building block of modern sequence‑to‑sequence and language models.'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=retriever_chain.invoke({\"input\":\"what is self-attention mechanism\"})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c19a1b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Self‑attention** (also called *intra‑attention*) is a core component of the Transformer architecture.  \\nIn a self‑attention layer every token (or position) in a sequence can “look at” every other token in the same sequence to gather contextual information.\\n\\nKey points:\\n\\n| Component | How it’s computed | What it represents |\\n|-----------|-------------------|--------------------|\\n| **Query (Q)** | Derived from the token’s own representation (e.g., by a linear projection) | The “request” or focus of the token |\\n| **Key (K)** | Derived from all tokens in the sequence (same linear projection as Q) | The “identifier” used to compare with queries |\\n| **Value (V)** | Derived from all tokens (same linear projection as K) | The actual information to be aggregated |\\n\\nThe attention score for a pair of tokens is the dot‑product of their Q and K vectors, usually scaled by the square root of the dimension and passed through a softmax to obtain a probability distribution. Each token then takes a weighted sum of all V vectors using these probabilities, producing a new representation that blends its own information with relevant context from the whole sequence.\\n\\nIn the Transformer, self‑attention is used both in the encoder (allowing each encoder position to attend to all other encoder positions) and in the decoder (where it is usually masked so that a position can only attend to positions **up to and including** itself, preventing future information leakage).\\n\\nThis mechanism enables the model to capture long‑range dependencies efficiently and is a fundamental building block of modern sequence‑to‑sequence and language models.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bc37637e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transformers are a deep‑learning model architecture that processes sequences by stacking multiple identical layers of two main components:\\n\\n1. **Multi‑head self‑attention** – allows each token to attend to every other token in the sequence, capturing long‑range dependencies without recurrence.  \\n2. **Position‑wise fully‑connected feed‑forward networks** – applied independently to each position, providing non‑linear transformations.\\n\\nEach of these sub‑layers is wrapped in a residual connection followed by layer normalization, ensuring stable training. The encoder stack (typically 6 layers in the original paper) feeds into a decoder stack that also uses self‑attention and cross‑attention to the encoder outputs. All sub‑layers and embedding layers produce outputs of the same dimensionality (d_model = 512 in the cited design). This architecture replaces traditional recurrent or convolutional networks for tasks such as machine translation.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=retriever_chain.invoke({\"input\":\"what are transformers\"})\n",
    "\n",
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0e9787",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
