{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3997da8",
   "metadata": {},
   "source": [
    "### Load Environment Variables\n",
    "\n",
    "This cell loads environment variables required for the project:\n",
    "\n",
    "- GROQ_API_KEY → API key for Groq LLM inference\n",
    "\n",
    "- HF_TOKEN → Token for HuggingFace model download\n",
    "\n",
    "dotenv automatically reads variables from a .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35da7695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"HF_TOKEN\"]=os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41b610d",
   "metadata": {},
   "source": [
    "### Load PDF File\n",
    "\n",
    "This loads a PDF document using PyPDFLoader and converts each page into a Document object.\n",
    "- docs[0] prints the first page for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac937744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../research_papers/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader=PyPDFLoader(\"../research_papers/attention.pdf\") # Replace with your file path\n",
    "docs=loader.load()\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f8544b",
   "metadata": {},
   "source": [
    "### Split Text into Chunks\n",
    "\n",
    "This cell splits the long PDF text into overlapping chunks:\n",
    "\n",
    "- chunk_size = 1000 characters\n",
    "\n",
    "- chunk_overlap = 250 characters\n",
    "\n",
    "Chunking improves embedding quality and retrieval accuracy in RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fa1fd52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../research_papers/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../research_papers/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='performing models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../research_papers/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='best models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=250)\n",
    "documents=text_splitter.split_documents(documents=docs)\n",
    "documents[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6ffc85",
   "metadata": {},
   "source": [
    "### Load Embedding Model\n",
    "\n",
    "Initializes the popular sentence-transformer model all-MiniLM-L6-v2 for embedding document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8caf7133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mouli\\AppData\\Local\\Temp\\ipykernel_49680\\780278957.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de85c56",
   "metadata": {},
   "source": [
    "### Store Documents in Chroma Vector DB\n",
    "\n",
    "- This creates a Chroma vector database from your chunked documents and uses embeddings to store vector representations.\n",
    "- The last line runs a similarity search query to test retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cb4623e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'title': '', 'author': '', 'subject': '', 'keywords': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'trapped': '/False', 'producer': 'pdfTeX-1.40.25', 'source': '../research_papers/attention.pdf', 'page': 1, 'page_label': '2', 'total_pages': 15}, page_content='self-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       "  1.1027004718780518),\n",
       " (Document(metadata={'keywords': '', 'creator': 'LaTeX with hyperref', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'page_label': '5', 'total_pages': 15, 'creationdate': '2024-04-10T21:11:43+00:00', 'title': '', 'trapped': '/False', 'producer': 'pdfTeX-1.40.25', 'author': '', 'page': 4, 'source': '../research_papers/attention.pdf', 'moddate': '2024-04-10T21:11:43+00:00'}, page_content='3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward'),\n",
       "  1.1430299282073975),\n",
       " (Document(metadata={'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'trapped': '/False', 'page_label': '2', 'total_pages': 15, 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'producer': 'pdfTeX-1.40.25', 'source': '../research_papers/attention.pdf', 'title': '', 'moddate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'page': 1}, page_content='the number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].'),\n",
       "  1.1453992128372192),\n",
       " (Document(metadata={'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'total_pages': 15, 'page': 5, 'title': '', 'keywords': '', 'producer': 'pdfTeX-1.40.25', 'creationdate': '2024-04-10T21:11:43+00:00', 'trapped': '/False', 'subject': '', 'author': '', 'source': '../research_papers/attention.pdf', 'moddate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'page_label': '6'}, page_content='relative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can'),\n",
       "  1.1529815196990967)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vector_store=Chroma.from_documents(documents=documents,embedding=embeddings)\n",
    "\n",
    "vector_store.similarity_search_with_score(\" self-Attention mechanisam\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7e05da",
   "metadata": {},
   "source": [
    "### Initialize Groq LLM\n",
    "\n",
    "This initializes the Groq-hosted LLM GPT-OSS-20B.\n",
    "llm.invoke(\"hi\") tests whether the API key is working.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3fb5fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I help you today?', additional_kwargs={'reasoning_content': 'We have a conversation: user says \"hi\". We need to respond politely. There\\'s no special instruction. Just say hi back.'}, response_metadata={'token_usage': {'completion_tokens': 45, 'prompt_tokens': 72, 'total_tokens': 117, 'completion_time': 0.04523991, 'prompt_time': 0.003793385, 'queue_time': 0.045327169, 'total_time': 0.049033295, 'completion_tokens_details': {'reasoning_tokens': 27}}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e189667b30', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--7921af1d-0521-4613-a852-f64e4a39360e-0', usage_metadata={'input_tokens': 72, 'output_tokens': 45, 'total_tokens': 117})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm=ChatGroq(model=\"openai/gpt-oss-20b\",api_key=groq_api_key)\n",
    "\n",
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c5112a",
   "metadata": {},
   "source": [
    "### Create Prompt Template\n",
    "\n",
    "Defines the main prompt template used by the LLM.\n",
    "It includes:\n",
    "\n",
    "- System message: instructs the assistant\n",
    "\n",
    "- Human message: includes the user query {input}\n",
    "\n",
    "- Context placeholder {context} inserted by retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7392ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\" \\nYou are an helpful assistant document Q and A tasks. please provide the response to the user questions.\\nIn which context is from the retriever. If you don't know the answer simply say i don't know.\\n\\n\\nHere is the context: \\n\\n{context}\\n\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Designing  the prompt template for the llm\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=\"\"\" \n",
    "You are an helpful assistant document Q and A tasks. please provide the response to the user questions.\n",
    "In which context is from the retriever. If you don't know the answer simply say i don't know.\n",
    "\n",
    "\n",
    "Here is the context: \n",
    "\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "\n",
    "    (\"system\",prompt),\n",
    "    (\"human\",\"{input}\")\n",
    "]\n",
    ")\n",
    "\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7924babb",
   "metadata": {},
   "source": [
    "### Create Retriever\n",
    "\n",
    "Converts the vector store into a retriever that fetches relevant chunks based on similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9518fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vector_store.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abca1ef3",
   "metadata": {},
   "source": [
    "### Create Retrieval + Q&A Chain\n",
    "\n",
    "- create_stuff_documents_chain → feeds retrieved context into the prompt\n",
    "\n",
    "- create_retrieval_chain → binds retriever + Q&A chain into one pipeline\n",
    "\n",
    "This forms a standard RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37bd5ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_classic.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "qanda_chain=create_stuff_documents_chain(llm,prompt_template) # It creates a chain for passing a list of documents to a model\n",
    "\n",
    "retriever_chain=create_retrieval_chain(retriever,qanda_chain) # It creates a retrieval chain that retrieves documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10436612",
   "metadata": {},
   "source": [
    "### Ask Questions Without History\n",
    "\n",
    "- These cells test simple queries using the basic RAG chain (no chat history).\n",
    "- The retrieved context is used to answer the question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3328dffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Self‑attention (also called intra‑attention)** is a mechanism that lets every element (token) in a sequence “look at” all other elements of the same sequence to compute a new representation for itself.  \\n\\nIn the Transformer, each token is first projected into three vectors:\\n\\n* **Query (Q)** – what the token is looking for  \\n* **Key (K)** – what the token offers as context  \\n* **Value (V)** – the actual content that will be aggregated\\n\\nFor a token *i*, the attention score to token *j* is computed as the dot‑product of *Qᵢ* and *Kⱼ*, scaled and passed through a softmax. These scores weight the *V* vectors from all positions, and the weighted sum becomes the new representation for token *i*.  \\n\\nWhen this operation is applied to every token simultaneously, each position can attend to all other positions in the same layer. Multi‑head attention simply repeats this process with different linear projections, allowing the model to capture diverse patterns of interaction.\\n\\nThus, self‑attention provides a flexible, position‑agnostic way to model dependencies within a single sequence, which is especially useful for tasks like machine translation, summarization, and language modeling.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=retriever_chain.invoke({\"input\":\"what is self-attention mechanism\"})\n",
    "\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cd9b982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Attention Mechanism (in the context of Transformers)**  \\n\\nAn attention mechanism is a way for a model to focus on different parts of its input when producing each output. In the Transformer architecture it works as follows:\\n\\n1. **Query, Key, Value Vectors**  \\n   - Every token (word, sub‑word, etc.) is projected into three vectors: **query** (q), **key** (k), and **value** (v).  \\n   - The query comes from the position for which we want to compute an output.  \\n   - The keys and values come from all positions that the query may attend to (e.g., all tokens in the same layer or all tokens in the encoder for encoder‑decoder attention).\\n\\n2. **Similarity Scores**  \\n   - The similarity between a query and each key is computed (typically by a dot product).  \\n   - These scores are scaled (by the square root of the key dimension) to keep gradients stable.\\n\\n3. **Softmax Weighting**  \\n   - A softmax is applied to the similarity scores, turning them into a probability distribution that sums to 1.  \\n   - This gives a weight for each position, indicating how much the model should “pay attention” to that position.\\n\\n4. **Weighted Sum of Values**  \\n   - The output for the query position is the weighted sum of all value vectors, using the attention weights as coefficients.\\n\\n5. **Multi‑Head Attention**  \\n   - The process is repeated in parallel across several “heads,” each with its own set of linear projections.  \\n   - The outputs of all heads are concatenated and projected again to produce the final attention output.\\n\\n6. **Self‑Attention vs Encoder‑Decoder Attention**  \\n   - *Self‑attention* uses the same sequence for queries, keys, and values, allowing each token to attend to all others in the same layer.  \\n   - *Encoder‑decoder attention* uses queries from the decoder and keys/values from the encoder, letting each decoder token attend to any encoder token.\\n\\nThe attention mechanism thus lets the model dynamically weigh the influence of all other tokens when computing each token’s representation, enabling it to capture long‑range dependencies and complex relationships in the data.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=retriever_chain.invoke({\"input\":\"what is attention mechanism\"})\n",
    "\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0958f334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the excerpt the authors are comparing **how different neural‑network layers connect the positions in a sequence** and what that means for learning long‑range dependencies, training speed and parallelism.  \\nThe main differences are:\\n\\n| Layer type | Connectivity pattern | Path length (between any two positions) | Computational complexity | Parallelism | Typical use |\\n|------------|----------------------|----------------------------------------|--------------------------|-------------|-------------|\\n| **Self‑attention (Transformer)** | Every position attends to every other position in one operation | Constant (1 hop) | \\\\(O(n^2)\\\\) in memory, but **constant** number of sequential ops | Fully parallel (all queries, keys, values computed at once) | Machine translation, language modelling, etc. |\\n| **Recurrent (RNN/LSTM/GRU)** | Each position depends only on the previous one | Linear (n hops) | \\\\(O(n)\\\\) sequential ops | No parallelism across time steps | Classic seq‑to‑seq, speech, handwriting, etc. |\\n| **Convolutional (ConvS2S)** | Local receptive field that expands with depth | Linear in depth (e.g., each layer adds a fixed number of hops) | \\\\(O(n \\\\cdot \\\\text{kernel size})\\\\) | Parallel over the sequence | Image‑style seq‑to‑seq, some NLP tasks |\\n| **ByteNet (WaveNet‑style)** | Dilated convolutions that double the receptive field each layer | Logarithmic in depth (e.g., \\\\(O(\\\\log n)\\\\) hops) | \\\\(O(n)\\\\) but with fewer layers | Parallel over the sequence | Speech synthesis, audio, etc. |\\n\\n**Key take‑aways**\\n\\n1. **Path length** – Self‑attention gives a constant‑length path between any two positions, making it easy to capture long‑range dependencies. Recurrent layers need a linear chain of operations; ConvS2S and ByteNet grow linearly or logarithmically with depth.\\n2. **Sequential vs. parallel** – Recurrent layers must be run one step at a time; self‑attention, ConvS2S and ByteNet can process all positions simultaneously, so they are faster on modern hardware.\\n3. **Complexity trade‑off** – Self‑attention is \\\\(O(n^2)\\\\) in memory but constant in time; recurrent is \\\\(O(n)\\\\) in time but \\\\(O(1)\\\\) memory per step; convolutional/ByteNet sit between these extremes.\\n4. **Practical impact** – Because self‑attention can relate any two positions instantly, Transformers achieve state‑of‑the‑art translation and other tasks while training much faster than recurrent or convolutional baselines.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=retriever_chain.invoke({\"input\":\"what is what is the difference between them?\"}) # It fails to answer the question because, it doesn't  have the previous context\n",
    "\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5611fe22",
   "metadata": {},
   "source": [
    "## Adding Chat History\n",
    "\n",
    "### Import History Modules\n",
    "\n",
    "This imports:\n",
    "\n",
    "- create_history_aware_retriever → makes retriever understand chat context\n",
    "\n",
    "- MessagesPlaceholder → placeholder for conversation history inside prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf6cd7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains.history_aware_retriever import create_history_aware_retriever\n",
    "\n",
    "from langchain_core.prompts import MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33081c2",
   "metadata": {},
   "source": [
    "### Create History-Aware Prompt\n",
    "\n",
    "This prompt rewrites user queries using chat history.\n",
    "\n",
    "Example:\n",
    "User: \"Tell me more\" → becomes a fully contextual question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7986ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "contextualize_aware_system_prompt = (\n",
    "\"\"\" \n",
    "Use the chat history to answer the user questions.\n",
    "In which the user question may related to the context presented in the cchat history.\n",
    "Do not answer the question directly, change the formate of the question if needed.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "contextualize_q_prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",contextualize_aware_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\",\"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fa3466",
   "metadata": {},
   "source": [
    "### Create History-Aware Retriever\n",
    "\n",
    "This retriever considers chat history before retrieving documents.\n",
    "\n",
    "This is essential for follow-up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "504c5a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "| VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x0000013B2A05F790>, search_kwargs={}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000013B4191ADE0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=' \\nUse the chat history to answer the user questions.\\nIn which the user question may related to the context presented in the cchat history.\\nDo not answer the question directly, change the formate of the question if needed.\\n'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000013B39341150>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000013B3947D790>, model_name='openai/gpt-oss-20b', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()\n",
       "| VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x0000013B2A05F790>, search_kwargs={})), kwargs={}, config={'run_name': 'chat_retriever_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_aware_retriver=create_history_aware_retriever(llm,retriever,contextualize_q_prompt)\n",
    "history_aware_retriver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cd0e82",
   "metadata": {},
   "source": [
    "### Build RAG Chain With History\n",
    "\n",
    "A new prompt that includes previous messages for context-aware generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d31ae446",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\",\"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ea2dcb",
   "metadata": {},
   "source": [
    "### Final RAG Chain (History-Aware)\n",
    "\n",
    "This creates a history-aware retrieval-augmented generation chain, the core of conversational RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "043912f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain=create_stuff_documents_chain(llm,qa_prompt)\n",
    "\n",
    "rag_chain=create_retrieval_chain(history_aware_retriver,question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d32687",
   "metadata": {},
   "source": [
    "### Test Conversational RAG\n",
    "\n",
    "Simulates a natural conversation:\n",
    "\n",
    "1. User asks a question\n",
    "\n",
    "2. RAG retrieves context\n",
    "\n",
    "3. Chat history is updated\n",
    "\n",
    "4. User asks a follow-up\n",
    "\n",
    "5. History-aware retriever answers correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e37417d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Self‑attention** (also called *intra‑attention*) is a mechanism in the Transformer that lets every token in a sequence gather information from *all other tokens in the same sequence*.\n",
      "\n",
      "- **Inputs**:  \n",
      "  - **Queries** – the vector for the token we’re updating.  \n",
      "  - **Keys** – the same set of vectors that represent all tokens.  \n",
      "  - **Values** – the same set of vectors that hold the information to be aggregated.\n",
      "\n",
      "- **Operation**:  \n",
      "  1. Compute a similarity score between the query and each key (typically a dot product).  \n",
      "  2. Apply a soft‑max to obtain attention weights.  \n",
      "  3. Take a weighted sum of the values using those weights.  \n",
      "  The result is a new representation for the query token that incorporates context from the whole sequence.\n",
      "\n",
      "- **Why it matters**:  \n",
      "  * In the encoder, every input token can attend to every other input token, capturing long‑range dependencies directly.  \n",
      "  * In the decoder, self‑attention is masked so each position only looks at previous positions, preserving the autoregressive property.\n",
      "\n",
      "Thus, self‑attention is the core operation that allows the Transformer to model relationships within a single sequence without recurrence or convolution.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage,HumanMessage\n",
    "chat_history=[]\n",
    "question=\"What is Self attention mechanism?\"\n",
    "response1=rag_chain.invoke({\"input\":question,\"chat_history\":chat_history})\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=response1[\"answer\"])\n",
    "    ]\n",
    ")\n",
    "\n",
    "question2=\"Tell me more about it?\"\n",
    "response2=rag_chain.invoke({\"input\":question,\"chat_history\":chat_history})\n",
    "print(response2['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bf11a9",
   "metadata": {},
   "source": [
    "### Add Multi-Session Chat History\n",
    "\n",
    "Imports utilities for multi-session chat history:\n",
    "\n",
    "- Each session gets its own message buffer\n",
    "\n",
    "- Required when building a Streamlit chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f1d7ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b403f6a7",
   "metadata": {},
   "source": [
    "### Session ID → History Mapping\n",
    "\n",
    "Creates a dictionary store that maps each session to its message history.\n",
    "\n",
    "Used in RunnableWithMessageHistory to maintain separate conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9493f624",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39a5c92",
   "metadata": {},
   "source": [
    "### Build Final Conversational RAG Chain\n",
    "\n",
    "This wraps the RAG chain with chat history, enabling:\n",
    "\n",
    "- Memory\n",
    "\n",
    "- Multi-session support\n",
    "\n",
    "- Cleaner conversation flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2efeef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92de5bd",
   "metadata": {},
   "source": [
    "### Test Multi-Session Chat\n",
    "\n",
    "Tests whether:\n",
    "\n",
    "- Session ID works\n",
    "\n",
    "- History persists\n",
    "\n",
    "- Answers use previous context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fdf82190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Self‑attention (also called intra‑attention) is an attention mechanism that operates **within a single sequence**.  \\nIn a self‑attention layer every token in the sequence produces three vectors—**query, key, and value**—all derived from the same representation of that token. Each token then “attends” to every other token (or to itself) in the same sequence:\\n\\n1. **Queries** from a token are compared with **keys** from all tokens to produce attention scores.  \\n2. These scores are normalized (e.g., via softmax) to give a weighting for each token.  \\n3. The weighted sum of the **values** produces a new representation for the token.\\n\\nThus, each position in the sequence can incorporate information from all positions, allowing the model to capture long‑range dependencies efficiently. Self‑attention is a core component of Transformer architectures and has been successfully applied to tasks such as reading comprehension, summarization, textual entailment, and sentence representation learning.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is self-attention?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c28f3510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Self‑attention in a nutshell**\\n\\n| Feature | What it means | Why it matters |\\n|---------|---------------|----------------|\\n| **Single‑sequence focus** | All queries, keys, and values come from the *same* token sequence (e.g., the encoder’s hidden states). | Lets every word “talk” to every other word in the same sentence, capturing global context in one layer. |\\n| **Attention scores** | For each token *i*, we compute a score with every token *j* as  \\\\(\\\\text{score}_{ij}=q_i\\\\cdot k_j^T\\\\). | These scores are turned into a probability distribution (softmax) that tells how much *i* should look at *j*. |\\n| **Weighted sum** | The output for token *i* is \\\\(\\\\sum_j \\\\alpha_{ij} v_j\\\\). | The token’s new representation is a mixture of all tokens, weighted by relevance. |\\n| **Parallelism** | All tokens are processed simultaneously; no sequential recurrence. | Much faster than RNNs and can handle very long contexts. |\\n| **Complexity** | \\\\(O(n^2)\\\\) per layer for a sequence of length *n* (quadratic in sequence length). | Still cheaper than convolutions that need many layers to cover long distances; linear in *n* for convolutional models. |\\n\\n---\\n\\n### How it fits into the Transformer\\n\\n1. **Encoder self‑attention** – every position can attend to *all* positions in the previous encoder layer.  \\n2. **Decoder self‑attention** – like the encoder, but masked so that a position can only attend to earlier positions (no “future” information).  \\n3. **Encoder‑decoder attention** – the decoder queries attend to the encoder’s outputs, allowing the decoder to focus on relevant input positions.\\n\\nThe Transformer uses **multi‑head attention**: the same self‑attention operation is run *h* times in parallel, each with its own linear projections. The outputs are concatenated and projected again. This lets the model learn different “sub‑spaces” of relationships (e.g., one head might focus on syntactic roles, another on semantic similarity).\\n\\n---\\n\\n### Why self‑attention is powerful\\n\\n| Problem | Traditional solutions | Self‑attention solution |\\n|---------|-----------------------|--------------------------|\\n| **Long‑range dependencies** | RNNs suffer from vanishing gradients; convolutions need many layers. | Every token sees every other token in a single layer. |\\n| **Variable‑length sequences** | RNNs handle them naturally but are slow. | Attention is *independent* of sequence length in terms of parallelism. |\\n| **Parallel training** | RNNs are inherently sequential. | Self‑attention can be fully parallelized across tokens. |\\n| **Interpretability** | Hard to trace hidden state flow. | Attention weights give a clear view of which tokens influence each other. |\\n\\n---\\n\\n### Practical applications\\n\\n- **Reading comprehension & question answering** – attention highlights relevant context passages.  \\n- **Abstractive summarization** – focuses on key sentences or phrases.  \\n- **Textual entailment** – aligns hypothesis words with premise words.  \\n- **Sentence representation learning** – self‑attention aggregates token information into a fixed‑size vector.\\n\\n---\\n\\n### Key take‑aways\\n\\n* Self‑attention lets a model look at the entire sequence at once, learning rich, context‑aware representations.  \\n* It is the cornerstone of the Transformer architecture, enabling state‑of‑the‑art performance across NLP tasks.  \\n* Its quadratic cost in sequence length is mitigated by efficient implementations (e.g., sparse attention, linear‑time approximations) in modern frameworks.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"Tell me more about it?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
